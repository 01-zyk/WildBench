{
  "gpt-4o-2024-05-13": {
    "model": "gpt-4o-2024-05-13",
    "win_much": 81,
    "win": 295,
    "tie": 156,
    "lose": 305,
    "lose_much": 72,
    "total": 1024,
    "avg_len": 3229.848184818482,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 62,
        "win": 232,
        "tie": 129,
        "lose": 211,
        "lose_much": 45
      },
      "Information/Advice seeking": {
        "win_much": 37,
        "win": 149,
        "tie": 91,
        "lose": 121,
        "lose_much": 13
      },
      "Coding & Debugging": {
        "win_much": 20,
        "win": 49,
        "tie": 16,
        "lose": 70,
        "lose_much": 20
      },
      "Creative Tasks": {
        "win_much": 23,
        "win": 133,
        "tie": 72,
        "lose": 185,
        "lose_much": 41
      },
      "Math & Data Analysis": {
        "win_much": 34,
        "win": 92,
        "tie": 47,
        "lose": 74,
        "lose_much": 26
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": 0.04050073637702504,
      "Information/Advice seeking": 0.09245742092457421,
      "Coding & Debugging": -0.06,
      "Creative Tasks": -0.09691629955947137,
      "Math & Data Analysis": 0.06227106227106227
    },
    "reward": 0.00390625,
    "task_macro_reward": 0.016395977479119677,
    "K": 1500
  },
  "gemini-1.5-pro": {
    "model": "gemini-1.5-pro",
    "win_much": 79,
    "win": 289,
    "tie": 145,
    "lose": 276,
    "lose_much": 105,
    "total": 1023,
    "avg_len": 2887.162192393736,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 29,
        "win": 154,
        "tie": 74,
        "lose": 103,
        "lose_much": 47
      },
      "Coding & Debugging": {
        "win_much": 24,
        "win": 52,
        "tie": 27,
        "lose": 44,
        "lose_much": 13
      },
      "Planning & Reasoning": {
        "win_much": 57,
        "win": 224,
        "tie": 125,
        "lose": 195,
        "lose_much": 82
      },
      "Creative Tasks": {
        "win_much": 32,
        "win": 119,
        "tie": 84,
        "lose": 173,
        "lose_much": 53
      },
      "Math & Data Analysis": {
        "win_much": 30,
        "win": 85,
        "tie": 31,
        "lose": 82,
        "lose_much": 46
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": 0.018427518427518427,
      "Coding & Debugging": 0.09375,
      "Planning & Reasoning": -0.015373352855051245,
      "Creative Tasks": -0.10412147505422993,
      "Math & Data Analysis": -0.05291970802919708
    },
    "reward": -0.01906158357771261,
    "task_macro_reward": -0.0036947772898860637,
    "K": 1500
  },
  "gpt-4-0125-preview": {
    "model": "gpt-4-0125-preview",
    "win_much": 37,
    "win": 353,
    "tie": 181,
    "lose": 332,
    "lose_much": 91,
    "total": 1024,
    "avg_len": 3258.2434607645873,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 25,
        "win": 259,
        "tie": 139,
        "lose": 263,
        "lose_much": 68
      },
      "Information/Advice seeking": {
        "win_much": 19,
        "win": 170,
        "tie": 81,
        "lose": 140,
        "lose_much": 27
      },
      "Coding & Debugging": {
        "win_much": 7,
        "win": 60,
        "tie": 23,
        "lose": 88,
        "lose_much": 18
      },
      "Creative Tasks": {
        "win_much": 12,
        "win": 212,
        "tie": 115,
        "lose": 155,
        "lose_much": 28
      },
      "Math & Data Analysis": {
        "win_much": 13,
        "win": 81,
        "tie": 41,
        "lose": 103,
        "lose_much": 49
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.05968169761273209,
      "Information/Advice seeking": 0.016018306636155607,
      "Coding & Debugging": -0.12755102040816327,
      "Creative Tasks": 0.023946360153256706,
      "Math & Data Analysis": -0.16376306620209058
    },
    "reward": -0.04248046875,
    "task_macro_reward": -0.07598428857164977,
    "K": 1500
  },
  "gemini-1.5-flash": {
    "model": "gemini-1.5-flash",
    "win_much": 64,
    "win": 231,
    "tie": 103,
    "lose": 342,
    "lose_much": 133,
    "total": 1024,
    "avg_len": 2989.946162657503,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 47,
        "win": 180,
        "tie": 81,
        "lose": 267,
        "lose_much": 94
      },
      "Information/Advice seeking": {
        "win_much": 27,
        "win": 104,
        "tie": 52,
        "lose": 152,
        "lose_much": 65
      },
      "Coding & Debugging": {
        "win_much": 14,
        "win": 48,
        "tie": 15,
        "lose": 51,
        "lose_much": 29
      },
      "Creative Tasks": {
        "win_much": 20,
        "win": 111,
        "tie": 64,
        "lose": 189,
        "lose_much": 54
      },
      "Math & Data Analysis": {
        "win_much": 33,
        "win": 71,
        "tie": 25,
        "lose": 92,
        "lose_much": 51
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.1352765321375187,
      "Information/Advice seeking": -0.155,
      "Coding & Debugging": -0.10509554140127389,
      "Creative Tasks": -0.16666666666666666,
      "Math & Data Analysis": -0.10477941176470588
    },
    "reward": -0.12158203125,
    "task_macro_reward": -0.12856754481582477,
    "K": 1500
  },
  "Meta-Llama-3-70B-Instruct": {
    "model": "Meta-Llama-3-70B-Instruct",
    "win_much": 88,
    "win": 201,
    "tie": 95,
    "lose": 409,
    "lose_much": 170,
    "total": 1023,
    "avg_len": 2836.827622014538,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 60,
        "win": 152,
        "tie": 76,
        "lose": 317,
        "lose_much": 123
      },
      "Information/Advice seeking": {
        "win_much": 48,
        "win": 100,
        "tie": 46,
        "lose": 194,
        "lose_much": 45
      },
      "Creative Tasks": {
        "win_much": 35,
        "win": 123,
        "tie": 48,
        "lose": 231,
        "lose_much": 64
      },
      "Math & Data Analysis": {
        "win_much": 38,
        "win": 54,
        "tie": 30,
        "lose": 101,
        "lose_much": 62
      },
      "Coding & Debugging": {
        "win_much": 15,
        "win": 27,
        "tie": 20,
        "lose": 67,
        "lose_much": 52
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.19986263736263737,
      "Information/Advice seeking": -0.10161662817551963,
      "Creative Tasks": -0.1656686626746507,
      "Math & Data Analysis": -0.16666666666666666,
      "Coding & Debugging": -0.3149171270718232
    },
    "reward": -0.18181818181818182,
    "task_macro_reward": -0.197879753980167,
    "K": 1500
  },
  "Yi-1.5-34B-Chat": {
    "model": "Yi-1.5-34B-Chat",
    "win_much": 50,
    "win": 238,
    "tie": 144,
    "lose": 344,
    "lose_much": 185,
    "total": 1024,
    "avg_len": 3317.9281997918833,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 33,
        "win": 179,
        "tie": 129,
        "lose": 270,
        "lose_much": 121
      },
      "Information/Advice seeking": {
        "win_much": 28,
        "win": 120,
        "tie": 68,
        "lose": 161,
        "lose_much": 51
      },
      "Coding & Debugging": {
        "win_much": 4,
        "win": 38,
        "tie": 19,
        "lose": 61,
        "lose_much": 64
      },
      "Creative Tasks": {
        "win_much": 22,
        "win": 147,
        "tie": 89,
        "lose": 179,
        "lose_much": 66
      },
      "Math & Data Analysis": {
        "win_much": 16,
        "win": 56,
        "tie": 31,
        "lose": 96,
        "lose_much": 80
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.18237704918032788,
      "Information/Advice seeking": -0.10163551401869159,
      "Coding & Debugging": -0.3844086021505376,
      "Creative Tasks": -0.11928429423459244,
      "Math & Data Analysis": -0.3010752688172043
    },
    "reward": -0.18359375,
    "task_macro_reward": -0.23318310334988152,
    "K": 1500
  },
  "claude-3-opus-20240229": {
    "model": "claude-3-opus-20240229",
    "win_much": 70,
    "win": 204,
    "tie": 111,
    "lose": 410,
    "lose_much": 168,
    "total": 1024,
    "avg_len": 2426.6531671858775,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 44,
        "win": 158,
        "tie": 89,
        "lose": 322,
        "lose_much": 117
      },
      "Information/Advice seeking": {
        "win_much": 36,
        "win": 81,
        "tie": 66,
        "lose": 196,
        "lose_much": 50
      },
      "Coding & Debugging": {
        "win_much": 14,
        "win": 40,
        "tie": 9,
        "lose": 80,
        "lose_much": 42
      },
      "Creative Tasks": {
        "win_much": 22,
        "win": 111,
        "tie": 70,
        "lose": 200,
        "lose_much": 88
      },
      "Math & Data Analysis": {
        "win_much": 26,
        "win": 67,
        "tie": 25,
        "lose": 114,
        "lose_much": 56
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.21232876712328766,
      "Information/Advice seeking": -0.16666666666666666,
      "Coding & Debugging": -0.2594594594594595,
      "Creative Tasks": -0.225050916496945,
      "Math & Data Analysis": -0.1857638888888889
    },
    "reward": -0.1962890625,
    "task_macro_reward": -0.21070217011131787,
    "K": 1500
  },
  "Llama-3-Instruct-8B-SimPO": {
    "model": "Llama-3-Instruct-8B-SimPO",
    "win_much": 67,
    "win": 234,
    "tie": 119,
    "lose": 353,
    "lose_much": 224,
    "total": 1024,
    "avg_len": 2491.6830491474425,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 39,
        "win": 176,
        "tie": 110,
        "lose": 266,
        "lose_much": 166
      },
      "Information/Advice seeking": {
        "win_much": 40,
        "win": 132,
        "tie": 53,
        "lose": 167,
        "lose_much": 45
      },
      "Creative Tasks": {
        "win_much": 29,
        "win": 148,
        "tie": 73,
        "lose": 212,
        "lose_much": 63
      },
      "Math & Data Analysis": {
        "win_much": 18,
        "win": 45,
        "tie": 21,
        "lose": 85,
        "lose_much": 121
      },
      "Coding & Debugging": {
        "win_much": 11,
        "win": 29,
        "tie": 17,
        "lose": 65,
        "lose_much": 74
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.22721268163804492,
      "Information/Advice seeking": -0.05148741418764302,
      "Creative Tasks": -0.12571428571428572,
      "Math & Data Analysis": -0.4241379310344828,
      "Coding & Debugging": -0.413265306122449
    },
    "reward": -0.21142578125,
    "task_macro_reward": -0.27175373171163625,
    "K": 1500
  },
  "reka-core-20240501": {
    "model": "reka-core-20240501",
    "win_much": 48,
    "win": 216,
    "tie": 111,
    "lose": 383,
    "lose_much": 183,
    "total": 1024,
    "avg_len": 2568.916046758767,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 31,
        "win": 159,
        "tie": 85,
        "lose": 307,
        "lose_much": 135
      },
      "Information/Advice seeking": {
        "win_much": 24,
        "win": 95,
        "tie": 56,
        "lose": 174,
        "lose_much": 64
      },
      "Coding & Debugging": {
        "win_much": 10,
        "win": 31,
        "tie": 13,
        "lose": 76,
        "lose_much": 57
      },
      "Creative Tasks": {
        "win_much": 17,
        "win": 137,
        "tie": 73,
        "lose": 200,
        "lose_much": 62
      },
      "Math & Data Analysis": {
        "win_much": 21,
        "win": 56,
        "tie": 19,
        "lose": 97,
        "lose_much": 74
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.2482566248256625,
      "Information/Advice seeking": -0.19249394673123488,
      "Coding & Debugging": -0.3716577540106952,
      "Creative Tasks": -0.15644171779141106,
      "Math & Data Analysis": -0.2752808988764045
    },
    "reward": -0.21337890625,
    "task_macro_reward": -0.26218905619184657,
    "K": 1500
  },
  "yi-large": {
    "model": "yi-large",
    "win_much": 37,
    "win": 208,
    "tie": 155,
    "lose": 410,
    "lose_much": 161,
    "total": 1022,
    "avg_len": 2964.2966014418125,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 22,
        "win": 159,
        "tie": 129,
        "lose": 314,
        "lose_much": 116
      },
      "Information/Advice seeking": {
        "win_much": 19,
        "win": 97,
        "tie": 86,
        "lose": 176,
        "lose_much": 54
      },
      "Coding & Debugging": {
        "win_much": 8,
        "win": 24,
        "tie": 12,
        "lose": 96,
        "lose_much": 47
      },
      "Creative Tasks": {
        "win_much": 19,
        "win": 124,
        "tie": 89,
        "lose": 218,
        "lose_much": 64
      },
      "Math & Data Analysis": {
        "win_much": 10,
        "win": 70,
        "tie": 44,
        "lose": 97,
        "lose_much": 62
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.23175675675675675,
      "Information/Advice seeking": -0.1724537037037037,
      "Coding & Debugging": -0.40106951871657753,
      "Creative Tasks": -0.17898832684824903,
      "Math & Data Analysis": -0.2314487632508834
    },
    "reward": -0.22015655577299412,
    "task_macro_reward": -0.2535663709145132,
    "K": 1500
  },
  "deepseekv2-chat": {
    "model": "deepseekv2-chat",
    "win_much": 39,
    "win": 228,
    "tie": 124,
    "lose": 396,
    "lose_much": 188,
    "total": 1024,
    "avg_len": 2675.0984615384614,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 28,
        "win": 166,
        "tie": 101,
        "lose": 305,
        "lose_much": 142
      },
      "Information/Advice seeking": {
        "win_much": 25,
        "win": 95,
        "tie": 62,
        "lose": 186,
        "lose_much": 64
      },
      "Coding & Debugging": {
        "win_much": 6,
        "win": 39,
        "tie": 14,
        "lose": 74,
        "lose_much": 58
      },
      "Creative Tasks": {
        "win_much": 8,
        "win": 149,
        "tie": 91,
        "lose": 194,
        "lose_much": 70
      },
      "Math & Data Analysis": {
        "win_much": 15,
        "win": 66,
        "tie": 17,
        "lose": 116,
        "lose_much": 70
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.2473045822102426,
      "Information/Advice seeking": -0.19560185185185186,
      "Coding & Debugging": -0.36387434554973824,
      "Creative Tasks": -0.1650390625,
      "Math & Data Analysis": -0.28169014084507044
    },
    "reward": -0.2275390625,
    "task_macro_reward": -0.2630924742881113,
    "K": 1500
  },
  "Llama-3-Instruct-8B-SimPO-ExPO": {
    "model": "Llama-3-Instruct-8B-SimPO-ExPO",
    "win_much": 55,
    "win": 231,
    "tie": 134,
    "lose": 340,
    "lose_much": 236,
    "total": 1024,
    "avg_len": 2435.8112449799196,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 31,
        "win": 169,
        "tie": 110,
        "lose": 268,
        "lose_much": 178
      },
      "Information/Advice seeking": {
        "win_much": 31,
        "win": 134,
        "tie": 63,
        "lose": 162,
        "lose_much": 47
      },
      "Creative Tasks": {
        "win_much": 34,
        "win": 142,
        "tie": 85,
        "lose": 197,
        "lose_much": 67
      },
      "Math & Data Analysis": {
        "win_much": 16,
        "win": 45,
        "tie": 27,
        "lose": 80,
        "lose_much": 122
      },
      "Coding & Debugging": {
        "win_much": 7,
        "win": 29,
        "tie": 16,
        "lose": 61,
        "lose_much": 81
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.25992063492063494,
      "Information/Advice seeking": -0.06864988558352403,
      "Creative Tasks": -0.11523809523809524,
      "Math & Data Analysis": -0.42586206896551726,
      "Coding & Debugging": -0.4639175257731959
    },
    "reward": -0.22998046875,
    "task_macro_reward": -0.294174855599155,
    "K": 1500
  },
  "deepseek-coder-v2": {
    "model": "deepseek-coder-v2",
    "win_much": 34,
    "win": 219,
    "tie": 120,
    "lose": 393,
    "lose_much": 210,
    "total": 1023,
    "avg_len": 2642.035860655738,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 15,
        "win": 173,
        "tie": 96,
        "lose": 308,
        "lose_much": 149
      },
      "Information/Advice seeking": {
        "win_much": 21,
        "win": 97,
        "tie": 53,
        "lose": 188,
        "lose_much": 73
      },
      "Coding & Debugging": {
        "win_much": 5,
        "win": 42,
        "tie": 13,
        "lose": 70,
        "lose_much": 61
      },
      "Creative Tasks": {
        "win_much": 12,
        "win": 129,
        "tie": 95,
        "lose": 217,
        "lose_much": 63
      },
      "Math & Data Analysis": {
        "win_much": 13,
        "win": 63,
        "tie": 17,
        "lose": 100,
        "lose_much": 91
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.2719298245614035,
      "Information/Advice seeking": -0.22569444444444445,
      "Coding & Debugging": -0.36649214659685864,
      "Creative Tasks": -0.18410852713178294,
      "Math & Data Analysis": -0.3397887323943662
    },
    "reward": -0.2570869990224829,
    "task_macro_reward": -0.29055961257604535,
    "K": 1500
  },
  "claude-3-sonnet-20240229": {
    "model": "claude-3-sonnet-20240229",
    "win_much": 59,
    "win": 166,
    "tie": 103,
    "lose": 378,
    "lose_much": 248,
    "total": 1024,
    "avg_len": 2386.706498951782,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 41,
        "win": 126,
        "tie": 84,
        "lose": 311,
        "lose_much": 156
      },
      "Information/Advice seeking": {
        "win_much": 26,
        "win": 81,
        "tie": 55,
        "lose": 186,
        "lose_much": 76
      },
      "Creative Tasks": {
        "win_much": 21,
        "win": 89,
        "tie": 54,
        "lose": 195,
        "lose_much": 141
      },
      "Math & Data Analysis": {
        "win_much": 24,
        "win": 44,
        "tie": 25,
        "lose": 111,
        "lose_much": 77
      },
      "Coding & Debugging": {
        "win_much": 12,
        "win": 32,
        "tie": 11,
        "lose": 68,
        "lose_much": 56
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.2889972144846797,
      "Information/Advice seeking": -0.2417452830188679,
      "Creative Tasks": -0.346,
      "Math & Data Analysis": -0.30782918149466193,
      "Coding & Debugging": -0.3463687150837989
    },
    "reward": -0.2880859375,
    "task_macro_reward": -0.3043896393218803,
    "K": 1500
  },
  "Yi-1.5-9B-Chat": {
    "model": "Yi-1.5-9B-Chat",
    "win_much": 50,
    "win": 167,
    "tie": 131,
    "lose": 355,
    "lose_much": 267,
    "total": 1024,
    "avg_len": 3285.944329896907,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 33,
        "win": 145,
        "tie": 107,
        "lose": 273,
        "lose_much": 175
      },
      "Information/Advice seeking": {
        "win_much": 29,
        "win": 85,
        "tie": 67,
        "lose": 164,
        "lose_much": 88
      },
      "Coding & Debugging": {
        "win_much": 4,
        "win": 26,
        "tie": 15,
        "lose": 64,
        "lose_much": 78
      },
      "Creative Tasks": {
        "win_much": 15,
        "win": 84,
        "tie": 94,
        "lose": 204,
        "lose_much": 118
      },
      "Math & Data Analysis": {
        "win_much": 22,
        "win": 44,
        "tie": 27,
        "lose": 92,
        "lose_much": 93
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.2810368349249659,
      "Information/Advice seeking": -0.22748267898383373,
      "Coding & Debugging": -0.49732620320855614,
      "Creative Tasks": -0.31650485436893205,
      "Math & Data Analysis": -0.34172661870503596
    },
    "reward": -0.3037109375,
    "task_macro_reward": -0.3376029559982535,
    "K": 1500
  },
  "glm-4-9b-chat": {
    "model": "glm-4-9b-chat",
    "win_much": 23,
    "win": 180,
    "tie": 105,
    "lose": 374,
    "lose_much": 238,
    "total": 1022,
    "avg_len": 3152.6586956521737,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 15,
        "win": 129,
        "tie": 77,
        "lose": 316,
        "lose_much": 162
      },
      "Information/Advice seeking": {
        "win_much": 16,
        "win": 97,
        "tie": 58,
        "lose": 180,
        "lose_much": 63
      },
      "Coding & Debugging": {
        "win_much": 2,
        "win": 27,
        "tie": 7,
        "lose": 72,
        "lose_much": 70
      },
      "Creative Tasks": {
        "win_much": 10,
        "win": 101,
        "tie": 70,
        "lose": 206,
        "lose_much": 90
      },
      "Math & Data Analysis": {
        "win_much": 7,
        "win": 40,
        "tie": 21,
        "lose": 88,
        "lose_much": 111
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.3440629470672389,
      "Information/Advice seeking": -0.213768115942029,
      "Coding & Debugging": -0.5084269662921348,
      "Creative Tasks": -0.2777777777777778,
      "Math & Data Analysis": -0.4794007490636704
    },
    "reward": -0.30528375733855184,
    "task_macro_reward": -0.38158252778561436,
    "K": 1500
  },
  "Qwen1.5-72B-Chat": {
    "model": "Qwen1.5-72B-Chat",
    "win_much": 34,
    "win": 169,
    "tie": 150,
    "lose": 398,
    "lose_much": 249,
    "total": 1023,
    "avg_len": 2362.328,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 18,
        "win": 118,
        "tie": 120,
        "lose": 323,
        "lose_much": 181
      },
      "Information/Advice seeking": {
        "win_much": 14,
        "win": 78,
        "tie": 77,
        "lose": 201,
        "lose_much": 69
      },
      "Coding & Debugging": {
        "win_much": 8,
        "win": 26,
        "tie": 21,
        "lose": 74,
        "lose_much": 69
      },
      "Creative Tasks": {
        "win_much": 15,
        "win": 111,
        "tie": 94,
        "lose": 214,
        "lose_much": 94
      },
      "Math & Data Analysis": {
        "win_much": 13,
        "win": 34,
        "tie": 28,
        "lose": 101,
        "lose_much": 113
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.3493421052631579,
      "Information/Advice seeking": -0.265375854214123,
      "Coding & Debugging": -0.4292929292929293,
      "Creative Tasks": -0.2471590909090909,
      "Math & Data Analysis": -0.4619377162629758
    },
    "reward": -0.32209188660801563,
    "task_macro_reward": -0.36678215849999785,
    "K": 1500
  },
  "command-r-plus": {
    "model": "command-r-plus",
    "win_much": 39,
    "win": 143,
    "tie": 105,
    "lose": 331,
    "lose_much": 285,
    "total": 1024,
    "avg_len": 2626.579180509413,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 24,
        "win": 108,
        "tie": 91,
        "lose": 276,
        "lose_much": 190
      },
      "Information/Advice seeking": {
        "win_much": 19,
        "win": 74,
        "tie": 55,
        "lose": 176,
        "lose_much": 78
      },
      "Coding & Debugging": {
        "win_much": 7,
        "win": 16,
        "tie": 13,
        "lose": 41,
        "lose_much": 102
      },
      "Creative Tasks": {
        "win_much": 23,
        "win": 86,
        "tie": 66,
        "lose": 192,
        "lose_much": 71
      },
      "Math & Data Analysis": {
        "win_much": 10,
        "win": 38,
        "tie": 19,
        "lose": 80,
        "lose_much": 137
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.36284470246734396,
      "Information/Advice seeking": -0.2736318407960199,
      "Coding & Debugging": -0.6005586592178771,
      "Creative Tasks": -0.23059360730593606,
      "Math & Data Analysis": -0.5211267605633803
    },
    "reward": -0.33203125,
    "task_macro_reward": -0.4212804404700934,
    "K": 1500
  },
  "SELM-Llama-3-8B-Instruct-iter-3": {
    "model": "SELM-Llama-3-8B-Instruct-iter-3",
    "win_much": 47,
    "win": 169,
    "tie": 96,
    "lose": 382,
    "lose_much": 281,
    "total": 1023,
    "avg_len": 2773.337435897436,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 24,
        "win": 129,
        "tie": 74,
        "lose": 304,
        "lose_much": 205
      },
      "Information/Advice seeking": {
        "win_much": 27,
        "win": 91,
        "tie": 51,
        "lose": 187,
        "lose_much": 75
      },
      "Coding & Debugging": {
        "win_much": 5,
        "win": 16,
        "tie": 10,
        "lose": 50,
        "lose_much": 99
      },
      "Creative Tasks": {
        "win_much": 29,
        "win": 106,
        "tie": 73,
        "lose": 231,
        "lose_much": 85
      },
      "Math & Data Analysis": {
        "win_much": 14,
        "win": 42,
        "tie": 12,
        "lose": 84,
        "lose_much": 132
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.3648097826086957,
      "Information/Advice seeking": -0.22273781902552203,
      "Coding & Debugging": -0.6166666666666667,
      "Creative Tasks": -0.22614503816793893,
      "Math & Data Analysis": -0.4894366197183099
    },
    "reward": -0.33284457478005863,
    "task_macro_reward": -0.40938697733310164,
    "K": 1500
  },
  "Qwen2-72B-Instruct": {
    "model": "Qwen2-72B-Instruct",
    "win_much": 35,
    "win": 165,
    "tie": 98,
    "lose": 447,
    "lose_much": 237,
    "total": 1024,
    "avg_len": 2713.9643584521386,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 22,
        "win": 122,
        "tie": 78,
        "lose": 353,
        "lose_much": 168
      },
      "Information/Advice seeking": {
        "win_much": 14,
        "win": 85,
        "tie": 51,
        "lose": 216,
        "lose_much": 66
      },
      "Coding & Debugging": {
        "win_much": 3,
        "win": 27,
        "tie": 14,
        "lose": 77,
        "lose_much": 71
      },
      "Creative Tasks": {
        "win_much": 15,
        "win": 95,
        "tie": 55,
        "lose": 230,
        "lose_much": 119
      },
      "Math & Data Analysis": {
        "win_much": 15,
        "win": 42,
        "tie": 15,
        "lose": 140,
        "lose_much": 74
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.35195154777927323,
      "Information/Advice seeking": -0.27199074074074076,
      "Coding & Debugging": -0.484375,
      "Creative Tasks": -0.3336575875486381,
      "Math & Data Analysis": -0.3776223776223776
    },
    "reward": -0.3349609375,
    "task_macro_reward": -0.37172414703918755,
    "K": 1500
  },
  "Qwen1.5-72B-Chat-greedy": {
    "model": "Qwen1.5-72B-Chat-greedy",
    "win_much": 41,
    "win": 149,
    "tie": 119,
    "lose": 438,
    "lose_much": 252,
    "total": 1024,
    "avg_len": 2352.061061061061,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 29,
        "win": 93,
        "tie": 93,
        "lose": 363,
        "lose_much": 183
      },
      "Information/Advice seeking": {
        "win_much": 17,
        "win": 66,
        "tie": 62,
        "lose": 219,
        "lose_much": 72
      },
      "Coding & Debugging": {
        "win_much": 7,
        "win": 21,
        "tie": 11,
        "lose": 76,
        "lose_much": 82
      },
      "Creative Tasks": {
        "win_much": 14,
        "win": 113,
        "tie": 74,
        "lose": 239,
        "lose_much": 86
      },
      "Math & Data Analysis": {
        "win_much": 21,
        "win": 29,
        "tie": 19,
        "lose": 102,
        "lose_much": 119
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.3797634691195795,
      "Information/Advice seeking": -0.30160550458715596,
      "Coding & Debugging": -0.5203045685279187,
      "Creative Tasks": -0.25665399239543724,
      "Math & Data Analysis": -0.46379310344827585
    },
    "reward": -0.34716796875,
    "task_macro_reward": -0.40296291844750104,
    "K": 1500
  },
  "SELM-Zephyr-7B-iter-3": {
    "model": "SELM-Zephyr-7B-iter-3",
    "win_much": 48,
    "win": 167,
    "tie": 76,
    "lose": 342,
    "lose_much": 339,
    "total": 1024,
    "avg_len": 2614.185185185185,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 32,
        "win": 130,
        "tie": 63,
        "lose": 265,
        "lose_much": 249
      },
      "Information/Advice seeking": {
        "win_much": 24,
        "win": 103,
        "tie": 39,
        "lose": 167,
        "lose_much": 92
      },
      "Coding & Debugging": {
        "win_much": 10,
        "win": 17,
        "tie": 7,
        "lose": 39,
        "lose_much": 112
      },
      "Creative Tasks": {
        "win_much": 31,
        "win": 95,
        "tie": 49,
        "lose": 224,
        "lose_much": 118
      },
      "Math & Data Analysis": {
        "win_much": 10,
        "win": 37,
        "tie": 18,
        "lose": 66,
        "lose_much": 152
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.38497970230040596,
      "Information/Advice seeking": -0.23529411764705882,
      "Coding & Debugging": -0.6108108108108108,
      "Creative Tasks": -0.293036750483559,
      "Math & Data Analysis": -0.5530035335689046
    },
    "reward": -0.36962890625,
    "task_macro_reward": -0.4373395412738437,
    "K": 1500
  },
  "reka-flash-20240226": {
    "model": "reka-flash-20240226",
    "win_much": 41,
    "win": 112,
    "tie": 149,
    "lose": 333,
    "lose_much": 363,
    "total": 1024,
    "avg_len": 2084.224448897796,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 28,
        "win": 81,
        "tie": 111,
        "lose": 259,
        "lose_much": 279
      },
      "Information/Advice seeking": {
        "win_much": 18,
        "win": 49,
        "tie": 75,
        "lose": 186,
        "lose_much": 109
      },
      "Coding & Debugging": {
        "win_much": 10,
        "win": 14,
        "tie": 33,
        "lose": 35,
        "lose_much": 105
      },
      "Creative Tasks": {
        "win_much": 18,
        "win": 65,
        "tie": 86,
        "lose": 203,
        "lose_much": 154
      },
      "Math & Data Analysis": {
        "win_much": 15,
        "win": 36,
        "tie": 27,
        "lose": 63,
        "lose_much": 148
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.44854881266490765,
      "Information/Advice seeking": -0.36498855835240274,
      "Coding & Debugging": -0.5355329949238579,
      "Creative Tasks": -0.3897338403041825,
      "Math & Data Analysis": -0.5069204152249135
    },
    "reward": -0.42236328125,
    "task_macro_reward": -0.46038839219917754,
    "K": 1500
  },
  "claude-3-haiku-20240307": {
    "model": "claude-3-haiku-20240307",
    "win_much": 31,
    "win": 114,
    "tie": 99,
    "lose": 382,
    "lose_much": 333,
    "total": 1024,
    "avg_len": 2256.994786235662,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 16,
        "win": 92,
        "tie": 76,
        "lose": 321,
        "lose_much": 224
      },
      "Information/Advice seeking": {
        "win_much": 15,
        "win": 50,
        "tie": 63,
        "lose": 202,
        "lose_much": 100
      },
      "Coding & Debugging": {
        "win_much": 10,
        "win": 22,
        "tie": 13,
        "lose": 55,
        "lose_much": 84
      },
      "Creative Tasks": {
        "win_much": 11,
        "win": 53,
        "tie": 52,
        "lose": 210,
        "lose_much": 161
      },
      "Math & Data Analysis": {
        "win_much": 8,
        "win": 38,
        "tie": 20,
        "lose": 99,
        "lose_much": 119
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.44238683127572015,
      "Information/Advice seeking": -0.3744186046511628,
      "Coding & Debugging": -0.49184782608695654,
      "Creative Tasks": -0.46919917864476385,
      "Math & Data Analysis": -0.4982394366197183
    },
    "reward": -0.42578125,
    "task_macro_reward": -0.457440965469351,
    "K": 1500
  },
  "Starling-LM-7B-beta-ExPO": {
    "model": "Starling-LM-7B-beta-ExPO",
    "win_much": 37,
    "win": 128,
    "tie": 74,
    "lose": 437,
    "lose_much": 319,
    "total": 1024,
    "avg_len": 2733.646231155779,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 20,
        "win": 95,
        "tie": 54,
        "lose": 367,
        "lose_much": 220
      },
      "Information/Advice seeking": {
        "win_much": 21,
        "win": 58,
        "tie": 29,
        "lose": 244,
        "lose_much": 84
      },
      "Coding & Debugging": {
        "win_much": 8,
        "win": 12,
        "tie": 10,
        "lose": 62,
        "lose_much": 106
      },
      "Creative Tasks": {
        "win_much": 16,
        "win": 101,
        "tie": 59,
        "lose": 241,
        "lose_much": 109
      },
      "Math & Data Analysis": {
        "win_much": 8,
        "win": 21,
        "tie": 13,
        "lose": 101,
        "lose_much": 145
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.4444444444444444,
      "Information/Advice seeking": -0.3577981651376147,
      "Coding & Debugging": -0.6212121212121212,
      "Creative Tasks": -0.30988593155893535,
      "Math & Data Analysis": -0.6145833333333334
    },
    "reward": -0.42626953125,
    "task_macro_reward": -0.4921427999408198,
    "K": 1500
  },
  "neo_7b_instruct_v0.1": {
    "model": "neo_7b_instruct_v0.1",
    "win_much": 29,
    "win": 123,
    "tie": 94,
    "lose": 362,
    "lose_much": 351,
    "total": 1021,
    "avg_len": 3490.7632950990615,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 24,
        "win": 96,
        "tie": 76,
        "lose": 299,
        "lose_much": 240
      },
      "Information/Advice seeking": {
        "win_much": 13,
        "win": 64,
        "tie": 60,
        "lose": 196,
        "lose_much": 96
      },
      "Coding & Debugging": {
        "win_much": 3,
        "win": 11,
        "tie": 6,
        "lose": 49,
        "lose_much": 113
      },
      "Creative Tasks": {
        "win_much": 17,
        "win": 87,
        "tie": 53,
        "lose": 203,
        "lose_much": 142
      },
      "Math & Data Analysis": {
        "win_much": 8,
        "win": 24,
        "tie": 13,
        "lose": 92,
        "lose_much": 141
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.43197278911564624,
      "Information/Advice seeking": -0.3473193473193473,
      "Coding & Debugging": -0.7087912087912088,
      "Creative Tasks": -0.3645418326693227,
      "Math & Data Analysis": -0.6007194244604317
    },
    "reward": -0.4324191968658178,
    "task_macro_reward": -0.5093860103267489,
    "K": 1500
  },
  "mistral-large-2402": {
    "model": "mistral-large-2402",
    "win_much": 31,
    "win": 110,
    "tie": 103,
    "lose": 382,
    "lose_much": 347,
    "total": 1024,
    "avg_len": 2329.6156217882835,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 15,
        "win": 73,
        "tie": 81,
        "lose": 308,
        "lose_much": 259
      },
      "Information/Advice seeking": {
        "win_much": 17,
        "win": 46,
        "tie": 58,
        "lose": 183,
        "lose_much": 128
      },
      "Coding & Debugging": {
        "win_much": 7,
        "win": 16,
        "tie": 18,
        "lose": 48,
        "lose_much": 105
      },
      "Creative Tasks": {
        "win_much": 11,
        "win": 87,
        "tie": 60,
        "lose": 213,
        "lose_much": 132
      },
      "Math & Data Analysis": {
        "win_much": 9,
        "win": 25,
        "tie": 18,
        "lose": 107,
        "lose_much": 125
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.49116847826086957,
      "Information/Advice seeking": -0.41550925925925924,
      "Coding & Debugging": -0.5876288659793815,
      "Creative Tasks": -0.3658051689860835,
      "Math & Data Analysis": -0.5528169014084507
    },
    "reward": -0.44140625,
    "task_macro_reward": -0.49976464314475677,
    "K": 1500
  },
  "command-r": {
    "model": "command-r",
    "win_much": 27,
    "win": 103,
    "tie": 86,
    "lose": 375,
    "lose_much": 346,
    "total": 1024,
    "avg_len": 2481.4983991462113,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 12,
        "win": 72,
        "tie": 65,
        "lose": 301,
        "lose_much": 263
      },
      "Information/Advice seeking": {
        "win_much": 17,
        "win": 54,
        "tie": 44,
        "lose": 212,
        "lose_much": 93
      },
      "Coding & Debugging": {
        "win_much": 3,
        "win": 13,
        "tie": 10,
        "lose": 45,
        "lose_much": 113
      },
      "Creative Tasks": {
        "win_much": 14,
        "win": 59,
        "tie": 53,
        "lose": 234,
        "lose_much": 102
      },
      "Math & Data Analysis": {
        "win_much": 3,
        "win": 20,
        "tie": 24,
        "lose": 72,
        "lose_much": 162
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.5126227208976157,
      "Information/Advice seeking": -0.36904761904761907,
      "Coding & Debugging": -0.6847826086956522,
      "Creative Tasks": -0.37987012987012986,
      "Math & Data Analysis": -0.6583629893238434
    },
    "reward": -0.4443359375,
    "task_macro_reward": -0.5445877285249543,
    "K": 1500
  },
  "Meta-Llama-3-8B-Instruct": {
    "model": "Meta-Llama-3-8B-Instruct",
    "win_much": 36,
    "win": 101,
    "tie": 66,
    "lose": 403,
    "lose_much": 357,
    "total": 1024,
    "avg_len": 2693.446521287643,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 18,
        "win": 81,
        "tie": 55,
        "lose": 338,
        "lose_much": 240
      },
      "Information/Advice seeking": {
        "win_much": 19,
        "win": 57,
        "tie": 31,
        "lose": 212,
        "lose_much": 105
      },
      "Creative Tasks": {
        "win_much": 14,
        "win": 66,
        "tie": 42,
        "lose": 240,
        "lose_much": 147
      },
      "Math & Data Analysis": {
        "win_much": 12,
        "win": 25,
        "tie": 20,
        "lose": 81,
        "lose_much": 145
      },
      "Coding & Debugging": {
        "win_much": 9,
        "win": 12,
        "tie": 7,
        "lose": 51,
        "lose_much": 105
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.47882513661202186,
      "Information/Advice seeking": -0.3856132075471698,
      "Creative Tasks": -0.43222003929273084,
      "Math & Data Analysis": -0.568904593639576,
      "Coding & Debugging": -0.6277173913043478
    },
    "reward": -0.4609375,
    "task_macro_reward": -0.5112162957812653,
    "K": 1500
  },
  "Starling-LM-7B-beta": {
    "model": "Starling-LM-7B-beta",
    "win_much": 32,
    "win": 103,
    "tie": 87,
    "lose": 407,
    "lose_much": 357,
    "total": 1024,
    "avg_len": 2627.0,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 17,
        "win": 75,
        "tie": 59,
        "lose": 337,
        "lose_much": 266
      },
      "Information/Advice seeking": {
        "win_much": 18,
        "win": 40,
        "tie": 43,
        "lose": 221,
        "lose_much": 111
      },
      "Coding & Debugging": {
        "win_much": 7,
        "win": 18,
        "tie": 7,
        "lose": 49,
        "lose_much": 112
      },
      "Creative Tasks": {
        "win_much": 15,
        "win": 78,
        "tie": 67,
        "lose": 237,
        "lose_much": 127
      },
      "Math & Data Analysis": {
        "win_much": 6,
        "win": 19,
        "tie": 15,
        "lose": 93,
        "lose_much": 151
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.5039787798408488,
      "Information/Advice seeking": -0.4237875288683603,
      "Coding & Debugging": -0.6243523316062176,
      "Creative Tasks": -0.3654580152671756,
      "Math & Data Analysis": -0.6408450704225352
    },
    "reward": -0.4658203125,
    "task_macro_reward": -0.5323867846921494,
    "K": 1500
  },
  "Mixtral-8x7B-Instruct-v0.1": {
    "model": "Mixtral-8x7B-Instruct-v0.1",
    "win_much": 29,
    "win": 97,
    "tie": 92,
    "lose": 348,
    "lose_much": 401,
    "total": 1024,
    "avg_len": 2397.785935884178,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 21,
        "win": 66,
        "tie": 58,
        "lose": 277,
        "lose_much": 312
      },
      "Information/Advice seeking": {
        "win_much": 18,
        "win": 37,
        "tie": 50,
        "lose": 187,
        "lose_much": 138
      },
      "Coding & Debugging": {
        "win_much": 7,
        "win": 16,
        "tie": 13,
        "lose": 40,
        "lose_much": 112
      },
      "Creative Tasks": {
        "win_much": 10,
        "win": 63,
        "tie": 59,
        "lose": 203,
        "lose_much": 166
      },
      "Math & Data Analysis": {
        "win_much": 7,
        "win": 30,
        "tie": 16,
        "lose": 77,
        "lose_much": 151
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.5401907356948229,
      "Information/Advice seeking": -0.45348837209302323,
      "Coding & Debugging": -0.6223404255319149,
      "Creative Tasks": -0.45109780439121755,
      "Math & Data Analysis": -0.5960854092526691
    },
    "reward": -0.48583984375,
    "task_macro_reward": -0.5465176523707753,
    "K": 1500
  },
  "Yi-1.5-6B-Chat": {
    "model": "Yi-1.5-6B-Chat",
    "win_much": 34,
    "win": 85,
    "tie": 61,
    "lose": 315,
    "lose_much": 420,
    "total": 1024,
    "avg_len": 3020.95737704918,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 22,
        "win": 64,
        "tie": 49,
        "lose": 259,
        "lose_much": 301
      },
      "Information/Advice seeking": {
        "win_much": 20,
        "win": 39,
        "tie": 39,
        "lose": 156,
        "lose_much": 144
      },
      "Coding & Debugging": {
        "win_much": 3,
        "win": 11,
        "tie": 6,
        "lose": 40,
        "lose_much": 125
      },
      "Creative Tasks": {
        "win_much": 11,
        "win": 51,
        "tie": 32,
        "lose": 194,
        "lose_much": 181
      },
      "Math & Data Analysis": {
        "win_much": 14,
        "win": 18,
        "tie": 15,
        "lose": 82,
        "lose_much": 148
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.541726618705036,
      "Information/Advice seeking": -0.4585427135678392,
      "Coding & Debugging": -0.7378378378378379,
      "Creative Tasks": -0.5149253731343284,
      "Math & Data Analysis": -0.5992779783393501
    },
    "reward": -0.4892578125,
    "task_macro_reward": -0.5812541802892282,
    "K": 1500
  },
  "Mistral-7B-Instruct-v0.2": {
    "model": "Mistral-7B-Instruct-v0.2",
    "win_much": 20,
    "win": 75,
    "tie": 104,
    "lose": 330,
    "lose_much": 433,
    "total": 1024,
    "avg_len": 2538.962577962578,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 10,
        "win": 50,
        "tie": 86,
        "lose": 254,
        "lose_much": 335
      },
      "Information/Advice seeking": {
        "win_much": 13,
        "win": 38,
        "tie": 48,
        "lose": 175,
        "lose_much": 148
      },
      "Coding & Debugging": {
        "win_much": 6,
        "win": 9,
        "tie": 13,
        "lose": 41,
        "lose_much": 120
      },
      "Creative Tasks": {
        "win_much": 9,
        "win": 56,
        "tie": 71,
        "lose": 196,
        "lose_much": 171
      },
      "Math & Data Analysis": {
        "win_much": 2,
        "win": 12,
        "tie": 19,
        "lose": 74,
        "lose_much": 165
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.580952380952381,
      "Information/Advice seeking": -0.4822274881516588,
      "Coding & Debugging": -0.6878306878306878,
      "Creative Tasks": -0.46123260437375746,
      "Math & Data Analysis": -0.7132352941176471
    },
    "reward": -0.52783203125,
    "task_macro_reward": -0.6043429725420965,
    "K": 1500
  },
  "reka-edge": {
    "model": "reka-edge",
    "win_much": 30,
    "win": 81,
    "tie": 77,
    "lose": 332,
    "lose_much": 447,
    "total": 1024,
    "avg_len": 2354.7745604963807,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 12,
        "win": 55,
        "tie": 59,
        "lose": 253,
        "lose_much": 349
      },
      "Information/Advice seeking": {
        "win_much": 21,
        "win": 33,
        "tie": 35,
        "lose": 180,
        "lose_much": 148
      },
      "Coding & Debugging": {
        "win_much": 6,
        "win": 12,
        "tie": 9,
        "lose": 45,
        "lose_much": 124
      },
      "Creative Tasks": {
        "win_much": 16,
        "win": 53,
        "tie": 63,
        "lose": 204,
        "lose_much": 177
      },
      "Math & Data Analysis": {
        "win_much": 7,
        "win": 16,
        "tie": 11,
        "lose": 63,
        "lose_much": 183
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.5989010989010989,
      "Information/Advice seeking": -0.48081534772182255,
      "Coding & Debugging": -0.6862244897959183,
      "Creative Tasks": -0.46101364522417154,
      "Math & Data Analysis": -0.7125
    },
    "reward": -0.52978515625,
    "task_macro_reward": -0.608548710405721,
    "K": 1500
  },
  "dbrx-instruct@together": {
    "model": "dbrx-instruct@together",
    "win_much": 25,
    "win": 71,
    "tie": 86,
    "lose": 374,
    "lose_much": 422,
    "total": 1024,
    "avg_len": 2427.583844580777,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 12,
        "win": 48,
        "tie": 62,
        "lose": 294,
        "lose_much": 328
      },
      "Information/Advice seeking": {
        "win_much": 13,
        "win": 27,
        "tie": 51,
        "lose": 182,
        "lose_much": 159
      },
      "Coding & Debugging": {
        "win_much": 6,
        "win": 12,
        "tie": 11,
        "lose": 55,
        "lose_much": 110
      },
      "Creative Tasks": {
        "win_much": 13,
        "win": 45,
        "tie": 38,
        "lose": 221,
        "lose_much": 188
      },
      "Math & Data Analysis": {
        "win_much": 5,
        "win": 18,
        "tie": 16,
        "lose": 100,
        "lose_much": 144
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.5900537634408602,
      "Information/Advice seeking": -0.5173611111111112,
      "Coding & Debugging": -0.6469072164948454,
      "Creative Tasks": -0.5207920792079208,
      "Math & Data Analysis": -0.6360424028268551
    },
    "reward": -0.53564453125,
    "task_macro_reward": -0.5930963770133487,
    "K": 1500
  },
  "Llama-2-70b-chat-hf": {
    "model": "Llama-2-70b-chat-hf",
    "win_much": 28,
    "win": 76,
    "tie": 65,
    "lose": 342,
    "lose_much": 444,
    "total": 1024,
    "avg_len": 2865.6973821989527,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 10,
        "win": 55,
        "tie": 49,
        "lose": 273,
        "lose_much": 333
      },
      "Information/Advice seeking": {
        "win_much": 21,
        "win": 41,
        "tie": 39,
        "lose": 193,
        "lose_much": 136
      },
      "Coding & Debugging": {
        "win_much": 3,
        "win": 8,
        "tie": 6,
        "lose": 39,
        "lose_much": 126
      },
      "Creative Tasks": {
        "win_much": 14,
        "win": 51,
        "tie": 33,
        "lose": 218,
        "lose_much": 174
      },
      "Math & Data Analysis": {
        "win_much": 5,
        "win": 15,
        "tie": 13,
        "lose": 56,
        "lose_much": 194
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.6,
      "Information/Advice seeking": -0.4441860465116279,
      "Coding & Debugging": -0.760989010989011,
      "Creative Tasks": -0.4969387755102041,
      "Math & Data Analysis": -0.7402826855123675
    },
    "reward": -0.5361328125,
    "task_macro_reward": -0.6295290264756003,
    "K": 1500
  },
  "tulu-2-dpo-70b": {
    "model": "tulu-2-dpo-70b",
    "win_much": 24,
    "win": 72,
    "tie": 73,
    "lose": 346,
    "lose_much": 436,
    "total": 1024,
    "avg_len": 2434.3764458464775,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 14,
        "win": 49,
        "tie": 56,
        "lose": 268,
        "lose_much": 339
      },
      "Information/Advice seeking": {
        "win_much": 15,
        "win": 32,
        "tie": 41,
        "lose": 175,
        "lose_much": 159
      },
      "Coding & Debugging": {
        "win_much": 5,
        "win": 13,
        "tie": 9,
        "lose": 38,
        "lose_much": 118
      },
      "Creative Tasks": {
        "win_much": 12,
        "win": 38,
        "tie": 45,
        "lose": 222,
        "lose_much": 172
      },
      "Math & Data Analysis": {
        "win_much": 10,
        "win": 12,
        "tie": 15,
        "lose": 77,
        "lose_much": 165
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.5984848484848485,
      "Information/Advice seeking": -0.5106635071090048,
      "Coding & Debugging": -0.6857923497267759,
      "Creative Tasks": -0.5153374233128835,
      "Math & Data Analysis": -0.6720430107526881
    },
    "reward": -0.5361328125,
    "task_macro_reward": -0.6103572806830488,
    "K": 1500
  },
  "Nous-Hermes-2-Mixtral-8x7B-DPO": {
    "model": "Nous-Hermes-2-Mixtral-8x7B-DPO",
    "win_much": 26,
    "win": 75,
    "tie": 55,
    "lose": 342,
    "lose_much": 446,
    "total": 1023,
    "avg_len": 2477.95656779661,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 16,
        "win": 60,
        "tie": 45,
        "lose": 274,
        "lose_much": 314
      },
      "Information/Advice seeking": {
        "win_much": 12,
        "win": 27,
        "tie": 34,
        "lose": 181,
        "lose_much": 163
      },
      "Coding & Debugging": {
        "win_much": 4,
        "win": 18,
        "tie": 6,
        "lose": 40,
        "lose_much": 111
      },
      "Creative Tasks": {
        "win_much": 11,
        "win": 35,
        "tie": 36,
        "lose": 209,
        "lose_much": 201
      },
      "Math & Data Analysis": {
        "win_much": 12,
        "win": 25,
        "tie": 12,
        "lose": 72,
        "lose_much": 156
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.5712270803949224,
      "Information/Advice seeking": -0.5467625899280576,
      "Coding & Debugging": -0.659217877094972,
      "Creative Tasks": -0.5630081300813008,
      "Math & Data Analysis": -0.6046931407942239
    },
    "reward": -0.5410557184750733,
    "task_macro_reward": -0.5932268613043429,
    "K": 1500
  },
  "Hermes-2-Theta-Llama-3-8B": {
    "model": "Hermes-2-Theta-Llama-3-8B",
    "win_much": 24,
    "win": 72,
    "tie": 70,
    "lose": 379,
    "lose_much": 427,
    "total": 1023,
    "avg_len": 2510.2716049382716,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 15,
        "win": 55,
        "tie": 54,
        "lose": 300,
        "lose_much": 317
      },
      "Information/Advice seeking": {
        "win_much": 14,
        "win": 41,
        "tie": 40,
        "lose": 202,
        "lose_much": 135
      },
      "Coding & Debugging": {
        "win_much": 5,
        "win": 11,
        "tie": 8,
        "lose": 42,
        "lose_much": 117
      },
      "Creative Tasks": {
        "win_much": 7,
        "win": 35,
        "tie": 48,
        "lose": 235,
        "lose_much": 188
      },
      "Math & Data Analysis": {
        "win_much": 9,
        "win": 20,
        "tie": 12,
        "lose": 87,
        "lose_much": 157
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.5728744939271255,
      "Information/Advice seeking": -0.4664351851851852,
      "Coding & Debugging": -0.6967213114754098,
      "Creative Tasks": -0.5477582846003899,
      "Math & Data Analysis": -0.6368421052631579
    },
    "reward": -0.5439882697947214,
    "task_macro_reward": -0.5940804589636797,
    "K": 1500
  },
  "Qwen1.5-7B-Chat@together": {
    "model": "Qwen1.5-7B-Chat@together",
    "win_much": 23,
    "win": 86,
    "tie": 79,
    "lose": 349,
    "lose_much": 449,
    "total": 1023,
    "avg_len": 2426.2860040567953,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 19,
        "win": 58,
        "tie": 57,
        "lose": 291,
        "lose_much": 322
      },
      "Information/Advice seeking": {
        "win_much": 8,
        "win": 41,
        "tie": 37,
        "lose": 199,
        "lose_much": 151
      },
      "Coding & Debugging": {
        "win_much": 7,
        "win": 13,
        "tie": 5,
        "lose": 44,
        "lose_much": 122
      },
      "Creative Tasks": {
        "win_much": 11,
        "win": 66,
        "tie": 61,
        "lose": 218,
        "lose_much": 166
      },
      "Math & Data Analysis": {
        "win_much": 7,
        "win": 12,
        "tie": 18,
        "lose": 76,
        "lose_much": 172
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.5615796519410977,
      "Information/Advice seeking": -0.5091743119266054,
      "Coding & Debugging": -0.6832460732984293,
      "Creative Tasks": -0.4425287356321839,
      "Math & Data Analysis": -0.6912280701754386
    },
    "reward": -0.5449657869012707,
    "task_macro_reward": -0.5954652911469525,
    "K": 1500
  },
  "Phi-3-medium-128k-instruct": {
    "model": "Phi-3-medium-128k-instruct",
    "win_much": 17,
    "win": 55,
    "tie": 83,
    "lose": 292,
    "lose_much": 511,
    "total": 1024,
    "avg_len": 2394.3256784968685,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 8,
        "win": 35,
        "tie": 65,
        "lose": 233,
        "lose_much": 401
      },
      "Information/Advice seeking": {
        "win_much": 9,
        "win": 21,
        "tie": 34,
        "lose": 159,
        "lose_much": 197
      },
      "Coding & Debugging": {
        "win_much": 3,
        "win": 9,
        "tie": 23,
        "lose": 34,
        "lose_much": 124
      },
      "Creative Tasks": {
        "win_much": 11,
        "win": 33,
        "tie": 36,
        "lose": 179,
        "lose_much": 227
      },
      "Math & Data Analysis": {
        "win_much": 5,
        "win": 19,
        "tie": 22,
        "lose": 82,
        "lose_much": 152
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.6630727762803235,
      "Information/Advice seeking": -0.611904761904762,
      "Coding & Debugging": -0.6917098445595855,
      "Creative Tasks": -0.5946502057613169,
      "Math & Data Analysis": -0.6375
    },
    "reward": -0.59814453125,
    "task_macro_reward": -0.6476232198264932,
    "K": 1500
  },
  "gpt-3.5-turbo-0125": {
    "model": "gpt-3.5-turbo-0125",
    "win_much": 11,
    "win": 63,
    "tie": 112,
    "lose": 275,
    "lose_much": 534,
    "total": 1024,
    "avg_len": 1787.6793969849246,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 7,
        "win": 47,
        "tie": 95,
        "lose": 216,
        "lose_much": 390
      },
      "Information/Advice seeking": {
        "win_much": 6,
        "win": 25,
        "tie": 58,
        "lose": 130,
        "lose_much": 219
      },
      "Coding & Debugging": {
        "win_much": 3,
        "win": 14,
        "tie": 19,
        "lose": 26,
        "lose_much": 136
      },
      "Creative Tasks": {
        "win_much": 4,
        "win": 38,
        "tie": 69,
        "lose": 174,
        "lose_much": 235
      },
      "Math & Data Analysis": {
        "win_much": 3,
        "win": 21,
        "tie": 24,
        "lose": 67,
        "lose_much": 174
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.6192052980132451,
      "Information/Advice seeking": -0.6061643835616438,
      "Coding & Debugging": -0.702020202020202,
      "Creative Tasks": -0.575,
      "Math & Data Analysis": -0.671280276816609
    },
    "reward": -0.6142578125,
    "task_macro_reward": -0.6420956420054668,
    "K": 1500
  },
  "Phi-3-mini-128k-instruct": {
    "model": "Phi-3-mini-128k-instruct",
    "win_much": 18,
    "win": 49,
    "tie": 69,
    "lose": 306,
    "lose_much": 533,
    "total": 1023,
    "avg_len": 2187.168205128205,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 14,
        "win": 34,
        "tie": 57,
        "lose": 229,
        "lose_much": 410
      },
      "Information/Advice seeking": {
        "win_much": 6,
        "win": 22,
        "tie": 36,
        "lose": 137,
        "lose_much": 224
      },
      "Coding & Debugging": {
        "win_much": 7,
        "win": 8,
        "tie": 12,
        "lose": 56,
        "lose_much": 111
      },
      "Creative Tasks": {
        "win_much": 10,
        "win": 29,
        "tie": 35,
        "lose": 187,
        "lose_much": 248
      },
      "Math & Data Analysis": {
        "win_much": 7,
        "win": 14,
        "tie": 18,
        "lose": 69,
        "lose_much": 177
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.6633064516129032,
      "Information/Advice seeking": -0.648235294117647,
      "Coding & Debugging": -0.6597938144329897,
      "Creative Tasks": -0.6227897838899804,
      "Math & Data Analysis": -0.6929824561403509
    },
    "reward": -0.6290322580645161,
    "task_macro_reward": -0.6621068216939323,
    "K": 1500
  },
  "Llama-2-7b-chat-hf": {
    "model": "Llama-2-7b-chat-hf",
    "win_much": 18,
    "win": 46,
    "tie": 45,
    "lose": 250,
    "lose_much": 594,
    "total": 1024,
    "avg_len": 2676.4344176285413,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 6,
        "win": 37,
        "tie": 26,
        "lose": 201,
        "lose_much": 441
      },
      "Information/Advice seeking": {
        "win_much": 15,
        "win": 29,
        "tie": 27,
        "lose": 148,
        "lose_much": 208
      },
      "Coding & Debugging": {
        "win_much": 2,
        "win": 4,
        "tie": 4,
        "lose": 14,
        "lose_much": 155
      },
      "Creative Tasks": {
        "win_much": 10,
        "win": 31,
        "tie": 28,
        "lose": 165,
        "lose_much": 274
      },
      "Math & Data Analysis": {
        "win_much": 1,
        "win": 8,
        "tie": 7,
        "lose": 50,
        "lose_much": 211
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.7271448663853727,
      "Information/Advice seeking": -0.5913348946135831,
      "Coding & Debugging": -0.88268156424581,
      "Creative Tasks": -0.6515748031496063,
      "Math & Data Analysis": -0.8339350180505415
    },
    "reward": -0.662109375,
    "task_macro_reward": -0.7544080528473462,
    "K": 1500
  },
  "gemma-7b-it": {
    "model": "gemma-7b-it",
    "win_much": 13,
    "win": 30,
    "tie": 89,
    "lose": 173,
    "lose_much": 696,
    "total": 1024,
    "avg_len": 1706.4305694305694,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 7,
        "win": 22,
        "tie": 68,
        "lose": 127,
        "lose_much": 536
      },
      "Information/Advice seeking": {
        "win_much": 7,
        "win": 12,
        "tie": 45,
        "lose": 90,
        "lose_much": 285
      },
      "Coding & Debugging": {
        "win_much": 3,
        "win": 4,
        "tie": 17,
        "lose": 17,
        "lose_much": 157
      },
      "Creative Tasks": {
        "win_much": 10,
        "win": 18,
        "tie": 54,
        "lose": 129,
        "lose_much": 317
      },
      "Math & Data Analysis": {
        "win_much": 2,
        "win": 7,
        "tie": 14,
        "lose": 34,
        "lose_much": 233
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.7651315789473684,
      "Information/Advice seeking": -0.7220956719817767,
      "Coding & Debugging": -0.8106060606060606,
      "Creative Tasks": -0.6865530303030303,
      "Math & Data Analysis": -0.843103448275862
    },
    "reward": -0.73681640625,
    "task_macro_reward": -0.7766605003786623,
    "K": 1500
  },
  "gemma-2b-it": {
    "model": "gemma-2b-it",
    "win_much": 6,
    "win": 14,
    "tie": 47,
    "lose": 115,
    "lose_much": 818,
    "total": 1024,
    "avg_len": 1564.652,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 3,
        "win": 11,
        "tie": 37,
        "lose": 89,
        "lose_much": 619
      },
      "Information/Advice seeking": {
        "win_much": 2,
        "win": 5,
        "tie": 26,
        "lose": 50,
        "lose_much": 356
      },
      "Coding & Debugging": {
        "win_much": 1,
        "win": 2,
        "tie": 7,
        "lose": 8,
        "lose_much": 179
      },
      "Creative Tasks": {
        "win_much": 6,
        "win": 10,
        "tie": 21,
        "lose": 108,
        "lose_much": 383
      },
      "Math & Data Analysis": {
        "win_much": 1,
        "win": 2,
        "tie": 13,
        "lose": 14,
        "lose_much": 260
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.8629776021080369,
      "Information/Advice seeking": -0.857630979498861,
      "Coding & Debugging": -0.9187817258883249,
      "Creative Tasks": -0.8068181818181818,
      "Math & Data Analysis": -0.9137931034482759
    },
    "reward": -0.84228515625,
    "task_macro_reward": -0.879539812778863,
    "K": 1500
  }
}