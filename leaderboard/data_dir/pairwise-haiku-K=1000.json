{
  "gpt-4-turbo-2024-04-09": {
    "model": "gpt-4-turbo-2024-04-09",
    "win_much": 336,
    "win": 285,
    "tie": 181,
    "lose": 125,
    "lose_much": 32,
    "total": 1024,
    "avg_len": 2956.7309697601668,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 231,
        "win": 241,
        "tie": 148,
        "lose": 91,
        "lose_much": 18
      },
      "Information/Advice seeking": {
        "win_much": 115,
        "win": 143,
        "tie": 103,
        "lose": 54,
        "lose_much": 15
      },
      "Coding & Debugging": {
        "win_much": 80,
        "win": 45,
        "tie": 22,
        "lose": 30,
        "lose_much": 7
      },
      "Creative Tasks": {
        "win_much": 163,
        "win": 164,
        "tie": 93,
        "lose": 62,
        "lose_much": 5
      },
      "Math & Data Analysis": {
        "win_much": 119,
        "win": 66,
        "tie": 43,
        "lose": 43,
        "lose_much": 13
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": 0.3950617283950617,
      "Information/Advice seeking": 0.336046511627907,
      "Coding & Debugging": 0.4375,
      "Creative Tasks": 0.42915811088295686,
      "Math & Data Analysis": 0.4137323943661972
    },
    "reward": 0.375,
    "task_macro_reward": 0.4025941097827629,
    "K": 1000
  },
  "gpt-4o-2024-05-13": {
    "model": "gpt-4o-2024-05-13",
    "win_much": 342,
    "win": 202,
    "tie": 218,
    "lose": 107,
    "lose_much": 34,
    "total": 1024,
    "avg_len": 3211.889258028793,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 146,
        "win": 111,
        "tie": 126,
        "lose": 52,
        "lose_much": 15
      },
      "Coding & Debugging": {
        "win_much": 78,
        "win": 37,
        "tie": 32,
        "lose": 20,
        "lose_much": 6
      },
      "Planning & Reasoning": {
        "win_much": 248,
        "win": 163,
        "tie": 179,
        "lose": 63,
        "lose_much": 21
      },
      "Information/Advice seeking": {
        "win_much": 131,
        "win": 101,
        "tie": 115,
        "lose": 55,
        "lose_much": 9
      },
      "Math & Data Analysis": {
        "win_much": 141,
        "win": 49,
        "tie": 46,
        "lose": 25,
        "lose_much": 13
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": 0.3566666666666667,
      "Coding & Debugging": 0.4653179190751445,
      "Planning & Reasoning": 0.41097922848664686,
      "Information/Advice seeking": 0.35279805352798055,
      "Math & Data Analysis": 0.5109489051094891
    },
    "reward": 0.34716796875,
    "task_macro_reward": 0.4295383851716135,
    "K": 1000
  },
  "gemini-1.5-pro": {
    "model": "gemini-1.5-pro",
    "win_much": 297,
    "win": 280,
    "tie": 168,
    "lose": 98,
    "lose_much": 40,
    "total": 1024,
    "avg_len": 2843.5617214043036,
    "task_categorized_results": {
      "Information/Advice seeking": {
        "win_much": 111,
        "win": 147,
        "tie": 82,
        "lose": 48,
        "lose_much": 18
      },
      "Coding & Debugging": {
        "win_much": 76,
        "win": 29,
        "tie": 35,
        "lose": 11,
        "lose_much": 6
      },
      "Planning & Reasoning": {
        "win_much": 201,
        "win": 226,
        "tie": 149,
        "lose": 67,
        "lose_much": 33
      },
      "Creative Tasks": {
        "win_much": 141,
        "win": 151,
        "tie": 80,
        "lose": 54,
        "lose_much": 16
      },
      "Math & Data Analysis": {
        "win_much": 106,
        "win": 71,
        "tie": 48,
        "lose": 36,
        "lose_much": 17
      }
    },
    "task_categorized_rewards": {
      "Information/Advice seeking": 0.35098522167487683,
      "Coding & Debugging": 0.5031847133757962,
      "Planning & Reasoning": 0.3661242603550296,
      "Creative Tasks": 0.3925339366515837,
      "Math & Data Analysis": 0.3830935251798561
    },
    "reward": 0.33984375,
    "task_macro_reward": 0.40076432190697525,
    "K": 1000
  },
  "claude-3-opus-20240229": {
    "model": "claude-3-opus-20240229",
    "win_much": 225,
    "win": 396,
    "tie": 147,
    "lose": 149,
    "lose_much": 33,
    "total": 1024,
    "avg_len": 2401.081052631579,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 89,
        "win": 223,
        "tie": 78,
        "lose": 74,
        "lose_much": 11
      },
      "Coding & Debugging": {
        "win_much": 68,
        "win": 59,
        "tie": 22,
        "lose": 26,
        "lose_much": 7
      },
      "Planning & Reasoning": {
        "win_much": 150,
        "win": 291,
        "tie": 125,
        "lose": 124,
        "lose_much": 26
      },
      "Information/Advice seeking": {
        "win_much": 75,
        "win": 189,
        "tie": 79,
        "lose": 70,
        "lose_much": 14
      },
      "Math & Data Analysis": {
        "win_much": 95,
        "win": 101,
        "tie": 32,
        "lose": 45,
        "lose_much": 17
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": 0.32105263157894737,
      "Coding & Debugging": 0.4258241758241758,
      "Planning & Reasoning": 0.28980446927374304,
      "Information/Advice seeking": 0.2822014051522248,
      "Math & Data Analysis": 0.36551724137931035
    },
    "reward": 0.30810546875,
    "task_macro_reward": 0.33906097187762385,
    "K": 1000
  },
  "gpt-4-0125-preview": {
    "model": "gpt-4-0125-preview",
    "win_much": 318,
    "win": 222,
    "tie": 225,
    "lose": 138,
    "lose_much": 58,
    "total": 1024,
    "avg_len": 3200.6378772112384,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 208,
        "win": 178,
        "tie": 200,
        "lose": 110,
        "lose_much": 33
      },
      "Information/Advice seeking": {
        "win_much": 113,
        "win": 114,
        "tie": 121,
        "lose": 58,
        "lose_much": 23
      },
      "Coding & Debugging": {
        "win_much": 70,
        "win": 37,
        "tie": 30,
        "lose": 38,
        "lose_much": 13
      },
      "Creative Tasks": {
        "win_much": 164,
        "win": 133,
        "tie": 123,
        "lose": 52,
        "lose_much": 14
      },
      "Math & Data Analysis": {
        "win_much": 109,
        "win": 36,
        "tie": 62,
        "lose": 54,
        "lose_much": 25
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": 0.28669410150891633,
      "Information/Advice seeking": 0.27505827505827507,
      "Coding & Debugging": 0.300531914893617,
      "Creative Tasks": 0.39197530864197533,
      "Math & Data Analysis": 0.26223776223776224
    },
    "reward": 0.294921875,
    "task_macro_reward": 0.2940930365849375,
    "K": 1000
  },
  "Meta-Llama-3-70B-Instruct": {
    "model": "Meta-Llama-3-70B-Instruct",
    "win_much": 264,
    "win": 288,
    "tie": 159,
    "lose": 186,
    "lose_much": 48,
    "total": 1023,
    "avg_len": 2773.202116402116,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 142,
        "win": 148,
        "tie": 71,
        "lose": 100,
        "lose_much": 13
      },
      "Coding & Debugging": {
        "win_much": 54,
        "win": 50,
        "tie": 22,
        "lose": 42,
        "lose_much": 11
      },
      "Planning & Reasoning": {
        "win_much": 183,
        "win": 223,
        "tie": 134,
        "lose": 137,
        "lose_much": 38
      },
      "Information/Advice seeking": {
        "win_much": 101,
        "win": 131,
        "tie": 97,
        "lose": 81,
        "lose_much": 18
      },
      "Math & Data Analysis": {
        "win_much": 99,
        "win": 86,
        "tie": 40,
        "lose": 43,
        "lose_much": 18
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": 0.3227848101265823,
      "Coding & Debugging": 0.26256983240223464,
      "Planning & Reasoning": 0.2629370629370629,
      "Information/Advice seeking": 0.2523364485981308,
      "Math & Data Analysis": 0.3583916083916084
    },
    "reward": 0.26099706744868034,
    "task_macro_reward": 0.2889505579949024,
    "K": 1000
  },
  "reka-core-20240501": {
    "model": "reka-core-20240501",
    "win_much": 240,
    "win": 304,
    "tie": 160,
    "lose": 151,
    "lose_much": 66,
    "total": 1024,
    "avg_len": 2528.546145494028,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 166,
        "win": 232,
        "tie": 131,
        "lose": 124,
        "lose_much": 48
      },
      "Information/Advice seeking": {
        "win_much": 71,
        "win": 152,
        "tie": 88,
        "lose": 82,
        "lose_much": 18
      },
      "Coding & Debugging": {
        "win_much": 49,
        "win": 60,
        "tie": 24,
        "lose": 29,
        "lose_much": 21
      },
      "Creative Tasks": {
        "win_much": 136,
        "win": 167,
        "tie": 79,
        "lose": 62,
        "lose_much": 17
      },
      "Math & Data Analysis": {
        "win_much": 92,
        "win": 63,
        "tie": 38,
        "lose": 49,
        "lose_much": 27
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": 0.24536376604850213,
      "Information/Advice seeking": 0.2141119221411192,
      "Coding & Debugging": 0.23770491803278687,
      "Creative Tasks": 0.3720173535791757,
      "Math & Data Analysis": 0.26765799256505574
    },
    "reward": 0.24462890625,
    "task_macro_reward": 0.2574800525675328,
    "K": 1000
  },
  "gemini-1.5-flash": {
    "model": "gemini-1.5-flash",
    "win_much": 239,
    "win": 220,
    "tie": 192,
    "lose": 160,
    "lose_much": 56,
    "total": 1024,
    "avg_len": 2955.504036908881,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 165,
        "win": 174,
        "tie": 166,
        "lose": 120,
        "lose_much": 40
      },
      "Information/Advice seeking": {
        "win_much": 78,
        "win": 108,
        "tie": 103,
        "lose": 79,
        "lose_much": 28
      },
      "Coding & Debugging": {
        "win_much": 66,
        "win": 27,
        "tie": 31,
        "lose": 21,
        "lose_much": 11
      },
      "Creative Tasks": {
        "win_much": 100,
        "win": 127,
        "tie": 93,
        "lose": 93,
        "lose_much": 14
      },
      "Math & Data Analysis": {
        "win_much": 94,
        "win": 56,
        "tie": 51,
        "lose": 52,
        "lose_much": 23
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": 0.22857142857142856,
      "Information/Advice seeking": 0.16287878787878787,
      "Coding & Debugging": 0.3717948717948718,
      "Creative Tasks": 0.24121779859484777,
      "Math & Data Analysis": 0.2644927536231884
    },
    "reward": 0.2080078125,
    "task_macro_reward": 0.25883775585308016,
    "K": 1000
  },
  "yi-large": {
    "model": "yi-large",
    "win_much": 252,
    "win": 246,
    "tie": 205,
    "lose": 156,
    "lose_much": 87,
    "total": 1024,
    "avg_len": 2909.2536997885836,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 176,
        "win": 197,
        "tie": 178,
        "lose": 117,
        "lose_much": 57
      },
      "Information/Advice seeking": {
        "win_much": 87,
        "win": 119,
        "tie": 114,
        "lose": 69,
        "lose_much": 36
      },
      "Coding & Debugging": {
        "win_much": 48,
        "win": 44,
        "tie": 30,
        "lose": 38,
        "lose_much": 21
      },
      "Creative Tasks": {
        "win_much": 135,
        "win": 144,
        "tie": 90,
        "lose": 75,
        "lose_much": 32
      },
      "Math & Data Analysis": {
        "win_much": 93,
        "win": 69,
        "tie": 56,
        "lose": 38,
        "lose_much": 29
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": 0.2193103448275862,
      "Information/Advice seeking": 0.17882352941176471,
      "Coding & Debugging": 0.16574585635359115,
      "Creative Tasks": 0.28886554621848737,
      "Math & Data Analysis": 0.2789473684210526
    },
    "reward": 0.205078125,
    "task_macro_reward": 0.22164035021715417,
    "K": 1000
  },
  "Llama-3-Instruct-8B-SimPO-ExPO": {
    "model": "Llama-3-Instruct-8B-SimPO-ExPO",
    "win_much": 215,
    "win": 339,
    "tie": 134,
    "lose": 177,
    "lose_much": 105,
    "total": 1024,
    "avg_len": 2382.2061855670104,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 136,
        "win": 185,
        "tie": 76,
        "lose": 73,
        "lose_much": 20
      },
      "Coding & Debugging": {
        "win_much": 40,
        "win": 56,
        "tie": 15,
        "lose": 42,
        "lose_much": 36
      },
      "Planning & Reasoning": {
        "win_much": 135,
        "win": 270,
        "tie": 115,
        "lose": 139,
        "lose_much": 79
      },
      "Information/Advice seeking": {
        "win_much": 90,
        "win": 169,
        "tie": 79,
        "lose": 77,
        "lose_much": 16
      },
      "Math & Data Analysis": {
        "win_much": 55,
        "win": 80,
        "tie": 23,
        "lose": 69,
        "lose_much": 64
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": 0.3510204081632653,
      "Coding & Debugging": 0.0582010582010582,
      "Planning & Reasoning": 0.16463414634146342,
      "Information/Advice seeking": 0.27842227378190254,
      "Math & Data Analysis": -0.012027491408934709
    },
    "reward": 0.1865234375,
    "task_macro_reward": 0.14139847980822495,
    "K": 1000
  },
  "deepseekv2-chat": {
    "model": "deepseekv2-chat",
    "win_much": 217,
    "win": 307,
    "tie": 142,
    "lose": 200,
    "lose_much": 83,
    "total": 1024,
    "avg_len": 2611.6164383561645,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 145,
        "win": 252,
        "tie": 121,
        "lose": 149,
        "lose_much": 56
      },
      "Information/Advice seeking": {
        "win_much": 69,
        "win": 154,
        "tie": 74,
        "lose": 94,
        "lose_much": 35
      },
      "Coding & Debugging": {
        "win_much": 43,
        "win": 54,
        "tie": 16,
        "lose": 49,
        "lose_much": 24
      },
      "Creative Tasks": {
        "win_much": 114,
        "win": 164,
        "tie": 85,
        "lose": 85,
        "lose_much": 31
      },
      "Math & Data Analysis": {
        "win_much": 79,
        "win": 79,
        "tie": 31,
        "lose": 71,
        "lose_much": 26
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": 0.19432918395573998,
      "Information/Advice seeking": 0.15023474178403756,
      "Coding & Debugging": 0.11559139784946236,
      "Creative Tasks": 0.255741127348643,
      "Math & Data Analysis": 0.1993006993006993
    },
    "reward": 0.18310546875,
    "task_macro_reward": 0.17741115491270806,
    "K": 1000
  },
  "claude-3-sonnet-20240229": {
    "model": "claude-3-sonnet-20240229",
    "win_much": 181,
    "win": 343,
    "tie": 160,
    "lose": 189,
    "lose_much": 74,
    "total": 1023,
    "avg_len": 2350.0834213305175,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 138,
        "win": 254,
        "tie": 128,
        "lose": 153,
        "lose_much": 44
      },
      "Information/Advice seeking": {
        "win_much": 50,
        "win": 176,
        "tie": 85,
        "lose": 85,
        "lose_much": 26
      },
      "Creative Tasks": {
        "win_much": 72,
        "win": 179,
        "tie": 93,
        "lose": 105,
        "lose_much": 34
      },
      "Math & Data Analysis": {
        "win_much": 88,
        "win": 77,
        "tie": 40,
        "lose": 60,
        "lose_much": 22
      },
      "Coding & Debugging": {
        "win_much": 55,
        "win": 48,
        "tie": 18,
        "lose": 39,
        "lose_much": 19
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": 0.201534170153417,
      "Information/Advice seeking": 0.1646919431279621,
      "Creative Tasks": 0.15527950310559005,
      "Math & Data Analysis": 0.259581881533101,
      "Coding & Debugging": 0.22625698324022347
    },
    "reward": 0.1798631476050831,
    "task_macro_reward": 0.20864784141419163,
    "K": 1000
  },
  "deepseek-coder-v2": {
    "model": "deepseek-coder-v2",
    "win_much": 224,
    "win": 294,
    "tie": 142,
    "lose": 203,
    "lose_much": 88,
    "total": 1024,
    "avg_len": 2590.356466876972,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 152,
        "win": 243,
        "tie": 116,
        "lose": 145,
        "lose_much": 69
      },
      "Information/Advice seeking": {
        "win_much": 72,
        "win": 144,
        "tie": 67,
        "lose": 107,
        "lose_much": 36
      },
      "Coding & Debugging": {
        "win_much": 47,
        "win": 55,
        "tie": 17,
        "lose": 44,
        "lose_much": 22
      },
      "Creative Tasks": {
        "win_much": 119,
        "win": 151,
        "tie": 91,
        "lose": 94,
        "lose_much": 26
      },
      "Math & Data Analysis": {
        "win_much": 85,
        "win": 72,
        "tie": 31,
        "lose": 63,
        "lose_much": 36
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": 0.1820689655172414,
      "Information/Advice seeking": 0.12793427230046947,
      "Coding & Debugging": 0.16486486486486487,
      "Creative Tasks": 0.2525987525987526,
      "Math & Data Analysis": 0.18641114982578397
    },
    "reward": 0.17724609375,
    "task_macro_reward": 0.17802495602487312,
    "K": 1000
  },
  "Yi-1.5-34B-Chat": {
    "model": "Yi-1.5-34B-Chat",
    "win_much": 273,
    "win": 158,
    "tie": 238,
    "lose": 172,
    "lose_much": 99,
    "total": 1024,
    "avg_len": 3269.627659574468,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 183,
        "win": 143,
        "tie": 192,
        "lose": 134,
        "lose_much": 64
      },
      "Information/Advice seeking": {
        "win_much": 100,
        "win": 80,
        "tie": 132,
        "lose": 79,
        "lose_much": 33
      },
      "Coding & Debugging": {
        "win_much": 50,
        "win": 25,
        "tie": 33,
        "lose": 38,
        "lose_much": 35
      },
      "Creative Tasks": {
        "win_much": 157,
        "win": 84,
        "tie": 116,
        "lose": 86,
        "lose_much": 31
      },
      "Math & Data Analysis": {
        "win_much": 93,
        "win": 37,
        "tie": 62,
        "lose": 51,
        "lose_much": 38
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": 0.17248603351955308,
      "Information/Advice seeking": 0.15919811320754718,
      "Coding & Debugging": 0.04696132596685083,
      "Creative Tasks": 0.26371308016877637,
      "Math & Data Analysis": 0.1708185053380783
    },
    "reward": 0.1630859375,
    "task_macro_reward": 0.15214277737653756,
    "K": 1000
  },
  "Llama-3-Instruct-8B-SimPO": {
    "model": "Llama-3-Instruct-8B-SimPO",
    "win_much": 213,
    "win": 309,
    "tie": 153,
    "lose": 179,
    "lose_much": 113,
    "total": 1024,
    "avg_len": 2425.796277145812,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 135,
        "win": 165,
        "tie": 87,
        "lose": 76,
        "lose_much": 27
      },
      "Coding & Debugging": {
        "win_much": 34,
        "win": 51,
        "tie": 12,
        "lose": 54,
        "lose_much": 37
      },
      "Planning & Reasoning": {
        "win_much": 137,
        "win": 249,
        "tie": 126,
        "lose": 137,
        "lose_much": 86
      },
      "Information/Advice seeking": {
        "win_much": 93,
        "win": 154,
        "tie": 93,
        "lose": 68,
        "lose_much": 22
      },
      "Math & Data Analysis": {
        "win_much": 58,
        "win": 73,
        "tie": 37,
        "lose": 59,
        "lose_much": 64
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": 0.3112244897959184,
      "Coding & Debugging": -0.023936170212765957,
      "Planning & Reasoning": 0.145578231292517,
      "Information/Advice seeking": 0.2651162790697674,
      "Math & Data Analysis": 0.003436426116838488
    },
    "reward": 0.1611328125,
    "task_macro_reward": 0.1146494442711119,
    "K": 1000
  },
  "Qwen1.5-72B-Chat": {
    "model": "Qwen1.5-72B-Chat",
    "win_much": 193,
    "win": 316,
    "tie": 137,
    "lose": 225,
    "lose_much": 101,
    "total": 1024,
    "avg_len": 2306.2088477366256,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 114,
        "win": 175,
        "tie": 79,
        "lose": 92,
        "lose_much": 30
      },
      "Coding & Debugging": {
        "win_much": 41,
        "win": 52,
        "tie": 19,
        "lose": 49,
        "lose_much": 30
      },
      "Planning & Reasoning": {
        "win_much": 133,
        "win": 238,
        "tie": 116,
        "lose": 176,
        "lose_much": 78
      },
      "Information/Advice seeking": {
        "win_much": 69,
        "win": 151,
        "tie": 67,
        "lose": 115,
        "lose_much": 30
      },
      "Math & Data Analysis": {
        "win_much": 61,
        "win": 73,
        "tie": 28,
        "lose": 80,
        "lose_much": 49
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": 0.25612244897959185,
      "Coding & Debugging": 0.06544502617801047,
      "Planning & Reasoning": 0.11605937921727395,
      "Information/Advice seeking": 0.13194444444444445,
      "Math & Data Analysis": 0.029209621993127148
    },
    "reward": 0.13427734375,
    "task_macro_reward": 0.10372187333685765,
    "K": 1000
  },
  "Qwen1.5-72B-Chat-greedy": {
    "model": "Qwen1.5-72B-Chat-greedy",
    "win_much": 179,
    "win": 326,
    "tie": 144,
    "lose": 221,
    "lose_much": 102,
    "total": 1024,
    "avg_len": 2296.3991769547324,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 111,
        "win": 184,
        "tie": 85,
        "lose": 78,
        "lose_much": 32
      },
      "Coding & Debugging": {
        "win_much": 36,
        "win": 57,
        "tie": 16,
        "lose": 47,
        "lose_much": 35
      },
      "Planning & Reasoning": {
        "win_much": 114,
        "win": 252,
        "tie": 116,
        "lose": 191,
        "lose_much": 67
      },
      "Information/Advice seeking": {
        "win_much": 59,
        "win": 144,
        "tie": 79,
        "lose": 119,
        "lose_much": 31
      },
      "Math & Data Analysis": {
        "win_much": 44,
        "win": 87,
        "tie": 31,
        "lose": 81,
        "lose_much": 48
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": 0.2693877551020408,
      "Coding & Debugging": 0.031413612565445025,
      "Planning & Reasoning": 0.10472972972972973,
      "Information/Advice seeking": 0.09375,
      "Math & Data Analysis": -0.003436426116838488
    },
    "reward": 0.12646484375,
    "task_macro_reward": 0.08086571692484203,
    "K": 1000
  },
  "Qwen2-72B-Instruct": {
    "model": "Qwen2-72B-Instruct",
    "win_much": 207,
    "win": 253,
    "tie": 158,
    "lose": 226,
    "lose_much": 115,
    "total": 1024,
    "avg_len": 2669.078206465068,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 110,
        "win": 132,
        "tie": 85,
        "lose": 109,
        "lose_much": 48
      },
      "Coding & Debugging": {
        "win_much": 40,
        "win": 48,
        "tie": 20,
        "lose": 46,
        "lose_much": 34
      },
      "Planning & Reasoning": {
        "win_much": 142,
        "win": 184,
        "tie": 145,
        "lose": 175,
        "lose_much": 82
      },
      "Information/Advice seeking": {
        "win_much": 70,
        "win": 121,
        "tie": 89,
        "lose": 109,
        "lose_much": 39
      },
      "Math & Data Analysis": {
        "win_much": 79,
        "win": 73,
        "tie": 44,
        "lose": 55,
        "lose_much": 35
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": 0.1518595041322314,
      "Coding & Debugging": 0.03723404255319149,
      "Planning & Reasoning": 0.0885989010989011,
      "Information/Advice seeking": 0.08644859813084112,
      "Math & Data Analysis": 0.1853146853146853
    },
    "reward": 0.10302734375,
    "task_macro_reward": 0.10534745664572215,
    "K": 1000
  },
  "SELM-Llama-3-8B-Instruct-iter-3": {
    "model": "SELM-Llama-3-8B-Instruct-iter-3",
    "win_much": 180,
    "win": 274,
    "tie": 173,
    "lose": 206,
    "lose_much": 118,
    "total": 1024,
    "avg_len": 2702.2344900105154,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 115,
        "win": 160,
        "tie": 94,
        "lose": 103,
        "lose_much": 16
      },
      "Coding & Debugging": {
        "win_much": 23,
        "win": 54,
        "tie": 17,
        "lose": 38,
        "lose_much": 44
      },
      "Planning & Reasoning": {
        "win_much": 118,
        "win": 199,
        "tie": 157,
        "lose": 160,
        "lose_much": 85
      },
      "Information/Advice seeking": {
        "win_much": 78,
        "win": 115,
        "tie": 110,
        "lose": 94,
        "lose_much": 30
      },
      "Math & Data Analysis": {
        "win_much": 50,
        "win": 70,
        "tie": 30,
        "lose": 73,
        "lose_much": 63
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": 0.2612704918032787,
      "Coding & Debugging": -0.07386363636363637,
      "Planning & Reasoning": 0.07301808066759388,
      "Information/Advice seeking": 0.13700234192037472,
      "Math & Data Analysis": -0.050699300699300696
    },
    "reward": 0.09375,
    "task_macro_reward": 0.0444659258029946,
    "K": 1000
  },
  "command-r-plus": {
    "model": "command-r-plus",
    "win_much": 174,
    "win": 210,
    "tie": 169,
    "lose": 217,
    "lose_much": 132,
    "total": 1024,
    "avg_len": 2618.987804878049,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 113,
        "win": 103,
        "tie": 96,
        "lose": 95,
        "lose_much": 27
      },
      "Coding & Debugging": {
        "win_much": 22,
        "win": 40,
        "tie": 29,
        "lose": 40,
        "lose_much": 47
      },
      "Planning & Reasoning": {
        "win_much": 120,
        "win": 165,
        "tie": 142,
        "lose": 164,
        "lose_much": 95
      },
      "Information/Advice seeking": {
        "win_much": 77,
        "win": 92,
        "tie": 97,
        "lose": 101,
        "lose_much": 36
      },
      "Math & Data Analysis": {
        "win_much": 44,
        "win": 56,
        "tie": 40,
        "lose": 80,
        "lose_much": 66
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": 0.2073732718894009,
      "Coding & Debugging": -0.1404494382022472,
      "Planning & Reasoning": 0.03717201166180758,
      "Information/Advice seeking": 0.09057071960297766,
      "Math & Data Analysis": -0.11888111888111888
    },
    "reward": 0.03759765625,
    "task_macro_reward": -0.009166859302038425,
    "K": 1000
  },
  "Yi-1.5-9B-Chat-Test": {
    "model": "Yi-1.5-9B-Chat-Test",
    "win_much": 194,
    "win": 158,
    "tie": 234,
    "lose": 223,
    "lose_much": 130,
    "total": 1022,
    "avg_len": 3232.0660276890308,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 149,
        "win": 130,
        "tie": 196,
        "lose": 158,
        "lose_much": 76
      },
      "Information/Advice seeking": {
        "win_much": 73,
        "win": 75,
        "tie": 124,
        "lose": 103,
        "lose_much": 53
      },
      "Coding & Debugging": {
        "win_much": 36,
        "win": 26,
        "tie": 33,
        "lose": 53,
        "lose_much": 31
      },
      "Creative Tasks": {
        "win_much": 103,
        "win": 98,
        "tie": 116,
        "lose": 114,
        "lose_much": 52
      },
      "Math & Data Analysis": {
        "win_much": 71,
        "win": 38,
        "tie": 59,
        "lose": 66,
        "lose_much": 41
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": 0.08321579689703808,
      "Information/Advice seeking": 0.014018691588785047,
      "Coding & Debugging": -0.04748603351955307,
      "Creative Tasks": 0.08902691511387163,
      "Math & Data Analysis": 0.05818181818181818
    },
    "reward": 0.030821917808219176,
    "task_macro_reward": 0.03772066822935273,
    "K": 1000
  },
  "Yi-1.5-9B-Chat": {
    "model": "Yi-1.5-9B-Chat",
    "win_much": 187,
    "win": 157,
    "tie": 242,
    "lose": 219,
    "lose_much": 134,
    "total": 1022,
    "avg_len": 3232.0660276890308,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 129,
        "win": 120,
        "tie": 218,
        "lose": 169,
        "lose_much": 73
      },
      "Information/Advice seeking": {
        "win_much": 71,
        "win": 73,
        "tie": 132,
        "lose": 99,
        "lose_much": 53
      },
      "Coding & Debugging": {
        "win_much": 32,
        "win": 30,
        "tie": 34,
        "lose": 47,
        "lose_much": 36
      },
      "Creative Tasks": {
        "win_much": 96,
        "win": 97,
        "tie": 120,
        "lose": 122,
        "lose_much": 48
      },
      "Math & Data Analysis": {
        "win_much": 75,
        "win": 41,
        "tie": 52,
        "lose": 63,
        "lose_much": 44
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": 0.04442877291960508,
      "Information/Advice seeking": 0.011682242990654205,
      "Coding & Debugging": -0.06983240223463687,
      "Creative Tasks": 0.07349896480331262,
      "Math & Data Analysis": 0.07272727272727272
    },
    "reward": 0.021526418786692758,
    "task_macro_reward": 0.023098222508175368,
    "K": 1000
  },
  "glm-4-9b-chat": {
    "model": "glm-4-9b-chat",
    "win_much": 162,
    "win": 178,
    "tie": 222,
    "lose": 219,
    "lose_much": 126,
    "total": 1023,
    "avg_len": 3111.403528114664,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 93,
        "win": 116,
        "tie": 111,
        "lose": 86,
        "lose_much": 50
      },
      "Coding & Debugging": {
        "win_much": 32,
        "win": 33,
        "tie": 30,
        "lose": 52,
        "lose_much": 28
      },
      "Planning & Reasoning": {
        "win_much": 104,
        "win": 142,
        "tie": 190,
        "lose": 167,
        "lose_much": 85
      },
      "Information/Advice seeking": {
        "win_much": 59,
        "win": 71,
        "tie": 134,
        "lose": 109,
        "lose_much": 39
      },
      "Math & Data Analysis": {
        "win_much": 51,
        "win": 54,
        "tie": 45,
        "lose": 65,
        "lose_much": 54
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": 0.12719298245614036,
      "Coding & Debugging": -0.03142857142857143,
      "Planning & Reasoning": 0.00944767441860465,
      "Information/Advice seeking": 0.0024271844660194173,
      "Math & Data Analysis": -0.031598513011152414
    },
    "reward": 0.015151515151515152,
    "task_macro_reward": 0.003155419591359269,
    "K": 1000
  },
  "Starling-LM-7B-beta-ExPO": {
    "model": "Starling-LM-7B-beta-ExPO",
    "win_much": 142,
    "win": 257,
    "tie": 173,
    "lose": 246,
    "lose_much": 147,
    "total": 1024,
    "avg_len": 2681.9740932642485,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 90,
        "win": 205,
        "tie": 145,
        "lose": 190,
        "lose_much": 104
      },
      "Information/Advice seeking": {
        "win_much": 50,
        "win": 121,
        "tie": 92,
        "lose": 129,
        "lose_much": 38
      },
      "Coding & Debugging": {
        "win_much": 26,
        "win": 46,
        "tie": 19,
        "lose": 46,
        "lose_much": 54
      },
      "Creative Tasks": {
        "win_much": 90,
        "win": 145,
        "tie": 108,
        "lose": 109,
        "lose_much": 36
      },
      "Math & Data Analysis": {
        "win_much": 36,
        "win": 60,
        "tie": 35,
        "lose": 88,
        "lose_much": 70
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.008855585831062671,
      "Information/Advice seeking": 0.018604651162790697,
      "Coding & Debugging": -0.14659685863874344,
      "Creative Tasks": 0.14754098360655737,
      "Math & Data Analysis": -0.16608996539792387
    },
    "reward": 0.00048828125,
    "task_macro_reward": -0.05245162803336087,
    "K": 1000
  },
  "mistral-large-2402": {
    "model": "mistral-large-2402",
    "win_much": 136,
    "win": 239,
    "tie": 147,
    "lose": 292,
    "lose_much": 139,
    "total": 1024,
    "avg_len": 2271.5561385099686,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 78,
        "win": 155,
        "tie": 74,
        "lose": 143,
        "lose_much": 26
      },
      "Coding & Debugging": {
        "win_much": 26,
        "win": 38,
        "tie": 27,
        "lose": 58,
        "lose_much": 39
      },
      "Planning & Reasoning": {
        "win_much": 92,
        "win": 161,
        "tie": 128,
        "lose": 223,
        "lose_much": 118
      },
      "Information/Advice seeking": {
        "win_much": 40,
        "win": 109,
        "tie": 73,
        "lose": 148,
        "lose_much": 58
      },
      "Math & Data Analysis": {
        "win_much": 51,
        "win": 55,
        "tie": 38,
        "lose": 75,
        "lose_much": 67
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": 0.12184873949579832,
      "Coding & Debugging": -0.12234042553191489,
      "Planning & Reasoning": -0.07894736842105263,
      "Information/Advice seeking": -0.08761682242990654,
      "Math & Data Analysis": -0.09090909090909091
    },
    "reward": -0.02880859375,
    "task_macro_reward": -0.0703826608981894,
    "K": 1000
  },
  "reka-flash-20240226": {
    "model": "reka-flash-20240226",
    "win_much": 127,
    "win": 269,
    "tie": 155,
    "lose": 238,
    "lose_much": 180,
    "total": 1024,
    "avg_len": 2034.6078431372548,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 73,
        "win": 205,
        "tie": 138,
        "lose": 179,
        "lose_much": 142
      },
      "Information/Advice seeking": {
        "win_much": 48,
        "win": 120,
        "tie": 79,
        "lose": 121,
        "lose_much": 62
      },
      "Coding & Debugging": {
        "win_much": 22,
        "win": 46,
        "tie": 30,
        "lose": 38,
        "lose_much": 55
      },
      "Creative Tasks": {
        "win_much": 71,
        "win": 150,
        "tie": 78,
        "lose": 138,
        "lose_much": 51
      },
      "Math & Data Analysis": {
        "win_much": 42,
        "win": 63,
        "tie": 42,
        "lose": 61,
        "lose_much": 82
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.07598371777476255,
      "Information/Advice seeking": -0.03372093023255814,
      "Coding & Debugging": -0.1518324607329843,
      "Creative Tasks": 0.05327868852459016,
      "Math & Data Analysis": -0.13448275862068965
    },
    "reward": -0.03662109375,
    "task_macro_reward": -0.08443249332983348,
    "K": 1000
  },
  "Starling-LM-7B-beta": {
    "model": "Starling-LM-7B-beta",
    "win_much": 130,
    "win": 250,
    "tie": 148,
    "lose": 259,
    "lose_much": 172,
    "total": 1024,
    "avg_len": 2562.4254431699687,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 87,
        "win": 187,
        "tie": 130,
        "lose": 201,
        "lose_much": 127
      },
      "Information/Advice seeking": {
        "win_much": 53,
        "win": 106,
        "tie": 78,
        "lose": 142,
        "lose_much": 49
      },
      "Coding & Debugging": {
        "win_much": 27,
        "win": 40,
        "tie": 12,
        "lose": 51,
        "lose_much": 58
      },
      "Creative Tasks": {
        "win_much": 83,
        "win": 149,
        "tie": 90,
        "lose": 115,
        "lose_much": 50
      },
      "Math & Data Analysis": {
        "win_much": 30,
        "win": 63,
        "tie": 33,
        "lose": 85,
        "lose_much": 74
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.06420765027322405,
      "Information/Advice seeking": -0.03271028037383177,
      "Coding & Debugging": -0.19414893617021275,
      "Creative Tasks": 0.1026694045174538,
      "Math & Data Analysis": -0.19298245614035087
    },
    "reward": -0.04541015625,
    "task_macro_reward": -0.09790865848305347,
    "K": 1000
  },
  "SELM-Zephyr-7B-iter-3": {
    "model": "SELM-Zephyr-7B-iter-3",
    "win_much": 152,
    "win": 215,
    "tie": 155,
    "lose": 242,
    "lose_much": 186,
    "total": 1024,
    "avg_len": 2567.4494736842107,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 107,
        "win": 133,
        "tie": 70,
        "lose": 125,
        "lose_much": 48
      },
      "Coding & Debugging": {
        "win_much": 13,
        "win": 27,
        "tie": 17,
        "lose": 54,
        "lose_much": 70
      },
      "Planning & Reasoning": {
        "win_much": 97,
        "win": 168,
        "tie": 140,
        "lose": 180,
        "lose_much": 139
      },
      "Information/Advice seeking": {
        "win_much": 68,
        "win": 112,
        "tie": 89,
        "lose": 104,
        "lose_much": 48
      },
      "Math & Data Analysis": {
        "win_much": 36,
        "win": 39,
        "tie": 45,
        "lose": 74,
        "lose_much": 91
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": 0.13043478260869565,
      "Coding & Debugging": -0.38950276243093923,
      "Planning & Reasoning": -0.06629834254143646,
      "Information/Advice seeking": 0.057007125890736345,
      "Math & Data Analysis": -0.2543859649122807
    },
    "reward": -0.04638671875,
    "task_macro_reward": -0.13750864884391453,
    "K": 1000
  },
  "Meta-Llama-3-8B-Instruct": {
    "model": "Meta-Llama-3-8B-Instruct",
    "win_much": 127,
    "win": 183,
    "tie": 162,
    "lose": 290,
    "lose_much": 182,
    "total": 1024,
    "avg_len": 2631.0074152542375,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 84,
        "win": 110,
        "tie": 83,
        "lose": 151,
        "lose_much": 53
      },
      "Coding & Debugging": {
        "win_much": 22,
        "win": 23,
        "tie": 22,
        "lose": 57,
        "lose_much": 54
      },
      "Planning & Reasoning": {
        "win_much": 79,
        "win": 140,
        "tie": 128,
        "lose": 242,
        "lose_much": 133
      },
      "Information/Advice seeking": {
        "win_much": 53,
        "win": 88,
        "tie": 95,
        "lose": 124,
        "lose_much": 63
      },
      "Math & Data Analysis": {
        "win_much": 42,
        "win": 36,
        "tie": 34,
        "lose": 96,
        "lose_much": 78
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": 0.02182952182952183,
      "Coding & Debugging": -0.2752808988764045,
      "Planning & Reasoning": -0.14542936288088643,
      "Information/Advice seeking": -0.06619385342789598,
      "Math & Data Analysis": -0.23076923076923078
    },
    "reward": -0.10595703125,
    "task_macro_reward": -0.1614594360895343,
    "K": 1000
  },
  "Mixtral-8x7B-Instruct-v0.1": {
    "model": "Mixtral-8x7B-Instruct-v0.1",
    "win_much": 118,
    "win": 190,
    "tie": 156,
    "lose": 301,
    "lose_much": 186,
    "total": 1024,
    "avg_len": 2357.1882229232388,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 62,
        "win": 113,
        "tie": 105,
        "lose": 138,
        "lose_much": 57
      },
      "Coding & Debugging": {
        "win_much": 25,
        "win": 33,
        "tie": 16,
        "lose": 46,
        "lose_much": 66
      },
      "Planning & Reasoning": {
        "win_much": 72,
        "win": 136,
        "tie": 124,
        "lose": 241,
        "lose_much": 150
      },
      "Information/Advice seeking": {
        "win_much": 40,
        "win": 89,
        "tie": 90,
        "lose": 153,
        "lose_much": 55
      },
      "Math & Data Analysis": {
        "win_much": 45,
        "win": 39,
        "tie": 31,
        "lose": 87,
        "lose_much": 81
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": -0.015789473684210527,
      "Coding & Debugging": -0.2553763440860215,
      "Planning & Reasoning": -0.18049792531120332,
      "Information/Advice seeking": -0.11007025761124122,
      "Math & Data Analysis": -0.21201413427561838
    },
    "reward": -0.12060546875,
    "task_macro_reward": -0.17410229223359563,
    "K": 1000
  },
  "command-r": {
    "model": "command-r",
    "win_much": 115,
    "win": 175,
    "tie": 165,
    "lose": 273,
    "lose_much": 200,
    "total": 1024,
    "avg_len": 2449.2974137931033,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 79,
        "win": 96,
        "tie": 92,
        "lose": 146,
        "lose_much": 37
      },
      "Coding & Debugging": {
        "win_much": 20,
        "win": 22,
        "tie": 19,
        "lose": 51,
        "lose_much": 70
      },
      "Planning & Reasoning": {
        "win_much": 73,
        "win": 148,
        "tie": 126,
        "lose": 209,
        "lose_much": 148
      },
      "Information/Advice seeking": {
        "win_much": 51,
        "win": 84,
        "tie": 102,
        "lose": 132,
        "lose_much": 52
      },
      "Math & Data Analysis": {
        "win_much": 21,
        "win": 45,
        "tie": 28,
        "lose": 75,
        "lose_much": 115
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": 0.03777777777777778,
      "Coding & Debugging": -0.3543956043956044,
      "Planning & Reasoning": -0.14985795454545456,
      "Information/Advice seeking": -0.05938242280285035,
      "Math & Data Analysis": -0.38380281690140844
    },
    "reward": -0.130859375,
    "task_macro_reward": -0.21137084282046223,
    "K": 1000
  },
  "neo_7b_instruct_v0.1": {
    "model": "neo_7b_instruct_v0.1",
    "win_much": 132,
    "win": 101,
    "tie": 246,
    "lose": 265,
    "lose_much": 193,
    "total": 1024,
    "avg_len": 3440.821771611526,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 85,
        "win": 90,
        "tie": 209,
        "lose": 209,
        "lose_much": 124
      },
      "Information/Advice seeking": {
        "win_much": 52,
        "win": 46,
        "tie": 137,
        "lose": 124,
        "lose_much": 63
      },
      "Coding & Debugging": {
        "win_much": 16,
        "win": 15,
        "tie": 27,
        "lose": 48,
        "lose_much": 70
      },
      "Creative Tasks": {
        "win_much": 94,
        "win": 62,
        "tie": 142,
        "lose": 130,
        "lose_much": 44
      },
      "Math & Data Analysis": {
        "win_much": 32,
        "win": 21,
        "tie": 55,
        "lose": 92,
        "lose_much": 83
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.13737796373779637,
      "Information/Advice seeking": -0.11848341232227488,
      "Coding & Debugging": -0.4005681818181818,
      "Creative Tasks": 0.03389830508474576,
      "Math & Data Analysis": -0.30565371024734983
    },
    "reward": -0.1396484375,
    "task_macro_reward": -0.21107950076380233,
    "K": 1000
  },
  "dbrx-instruct@together": {
    "model": "dbrx-instruct@together",
    "win_much": 117,
    "win": 191,
    "tie": 132,
    "lose": 301,
    "lose_much": 216,
    "total": 1024,
    "avg_len": 2353.0052246603973,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 65,
        "win": 117,
        "tie": 70,
        "lose": 134,
        "lose_much": 90
      },
      "Coding & Debugging": {
        "win_much": 23,
        "win": 35,
        "tie": 23,
        "lose": 55,
        "lose_much": 54
      },
      "Planning & Reasoning": {
        "win_much": 74,
        "win": 143,
        "tie": 111,
        "lose": 247,
        "lose_much": 155
      },
      "Information/Advice seeking": {
        "win_much": 37,
        "win": 77,
        "tie": 62,
        "lose": 168,
        "lose_much": 83
      },
      "Math & Data Analysis": {
        "win_much": 45,
        "win": 54,
        "tie": 32,
        "lose": 81,
        "lose_much": 73
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": -0.0703781512605042,
      "Coding & Debugging": -0.21578947368421053,
      "Planning & Reasoning": -0.1821917808219178,
      "Information/Advice seeking": -0.21428571428571427,
      "Math & Data Analysis": -0.1456140350877193
    },
    "reward": -0.150390625,
    "task_macro_reward": -0.17445479914308107,
    "K": 1000
  },
  "Hermes-2-Theta-Llama-3-8B": {
    "model": "Hermes-2-Theta-Llama-3-8B",
    "win_much": 102,
    "win": 183,
    "tie": 159,
    "lose": 293,
    "lose_much": 213,
    "total": 1023,
    "avg_len": 2450.127368421053,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 47,
        "win": 111,
        "tie": 97,
        "lose": 157,
        "lose_much": 68
      },
      "Coding & Debugging": {
        "win_much": 21,
        "win": 31,
        "tie": 18,
        "lose": 42,
        "lose_much": 68
      },
      "Planning & Reasoning": {
        "win_much": 67,
        "win": 127,
        "tie": 122,
        "lose": 235,
        "lose_much": 175
      },
      "Information/Advice seeking": {
        "win_much": 43,
        "win": 79,
        "tie": 84,
        "lose": 161,
        "lose_much": 60
      },
      "Math & Data Analysis": {
        "win_much": 34,
        "win": 50,
        "tie": 30,
        "lose": 83,
        "lose_much": 89
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": -0.09166666666666666,
      "Coding & Debugging": -0.2916666666666667,
      "Planning & Reasoning": -0.2231404958677686,
      "Information/Advice seeking": -0.1358313817330211,
      "Math & Data Analysis": -0.25
    },
    "reward": -0.16226783968719452,
    "task_macro_reward": -0.21517759025210592,
    "K": 1000
  },
  "neo_7b_instruct_v0.1-ExPO": {
    "model": "neo_7b_instruct_v0.1-ExPO",
    "win_much": 127,
    "win": 88,
    "tie": 236,
    "lose": 255,
    "lose_much": 212,
    "total": 1024,
    "avg_len": 3572.305010893246,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 74,
        "win": 76,
        "tie": 191,
        "lose": 203,
        "lose_much": 159
      },
      "Information/Advice seeking": {
        "win_much": 47,
        "win": 44,
        "tie": 142,
        "lose": 117,
        "lose_much": 63
      },
      "Creative Tasks": {
        "win_much": 108,
        "win": 49,
        "tie": 135,
        "lose": 118,
        "lose_much": 53
      },
      "Math & Data Analysis": {
        "win_much": 28,
        "win": 18,
        "tie": 49,
        "lose": 95,
        "lose_much": 93
      },
      "Coding & Debugging": {
        "win_much": 10,
        "win": 14,
        "tie": 14,
        "lose": 55,
        "lose_much": 76
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.2112375533428165,
      "Information/Advice seeking": -0.1271186440677966,
      "Creative Tasks": 0.04427645788336933,
      "Math & Data Analysis": -0.3657243816254417,
      "Coding & Debugging": -0.5118343195266272
    },
    "reward": -0.16455078125,
    "task_macro_reward": -0.2699569770977227,
    "K": 1000
  },
  "Nous-Hermes-2-Mixtral-8x7B-DPO": {
    "model": "Nous-Hermes-2-Mixtral-8x7B-DPO",
    "win_much": 113,
    "win": 163,
    "tie": 136,
    "lose": 296,
    "lose_much": 222,
    "total": 1023,
    "avg_len": 2423.65376344086,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 55,
        "win": 98,
        "tie": 85,
        "lose": 146,
        "lose_much": 82
      },
      "Coding & Debugging": {
        "win_much": 26,
        "win": 29,
        "tie": 23,
        "lose": 41,
        "lose_much": 59
      },
      "Planning & Reasoning": {
        "win_much": 79,
        "win": 112,
        "tie": 117,
        "lose": 231,
        "lose_much": 163
      },
      "Information/Advice seeking": {
        "win_much": 42,
        "win": 66,
        "tie": 65,
        "lose": 172,
        "lose_much": 72
      },
      "Math & Data Analysis": {
        "win_much": 43,
        "win": 48,
        "tie": 26,
        "lose": 74,
        "lose_much": 91
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": -0.10944206008583691,
      "Coding & Debugging": -0.21910112359550563,
      "Planning & Reasoning": -0.20441595441595442,
      "Information/Advice seeking": -0.19904076738609114,
      "Math & Data Analysis": -0.21631205673758866
    },
    "reward": -0.17155425219941348,
    "task_macro_reward": -0.19887438420789424,
    "K": 1000
  },
  "tulu-2-dpo-70b": {
    "model": "tulu-2-dpo-70b",
    "win_much": 103,
    "win": 181,
    "tie": 113,
    "lose": 312,
    "lose_much": 219,
    "total": 1024,
    "avg_len": 2393.4762931034484,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 66,
        "win": 116,
        "tie": 97,
        "lose": 251,
        "lose_much": 180
      },
      "Information/Advice seeking": {
        "win_much": 43,
        "win": 74,
        "tie": 57,
        "lose": 168,
        "lose_much": 75
      },
      "Coding & Debugging": {
        "win_much": 17,
        "win": 30,
        "tie": 13,
        "lose": 45,
        "lose_much": 72
      },
      "Creative Tasks": {
        "win_much": 61,
        "win": 125,
        "tie": 61,
        "lose": 162,
        "lose_much": 52
      },
      "Math & Data Analysis": {
        "win_much": 31,
        "win": 35,
        "tie": 28,
        "lose": 86,
        "lose_much": 103
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.2556338028169014,
      "Information/Advice seeking": -0.18944844124700239,
      "Coding & Debugging": -0.3531073446327684,
      "Creative Tasks": -0.020607375271149676,
      "Math & Data Analysis": -0.34452296819787986
    },
    "reward": -0.17724609375,
    "task_macro_reward": -0.259902796649467,
    "K": 1000
  },
  "reka-edge": {
    "model": "reka-edge",
    "win_much": 92,
    "win": 204,
    "tie": 135,
    "lose": 251,
    "lose_much": 256,
    "total": 1024,
    "avg_len": 2306.7473347547975,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 51,
        "win": 138,
        "tie": 112,
        "lose": 199,
        "lose_much": 208
      },
      "Information/Advice seeking": {
        "win_much": 38,
        "win": 93,
        "tie": 73,
        "lose": 118,
        "lose_much": 90
      },
      "Coding & Debugging": {
        "win_much": 16,
        "win": 31,
        "tie": 21,
        "lose": 49,
        "lose_much": 72
      },
      "Creative Tasks": {
        "win_much": 63,
        "win": 130,
        "tie": 74,
        "lose": 132,
        "lose_much": 77
      },
      "Math & Data Analysis": {
        "win_much": 22,
        "win": 37,
        "tie": 24,
        "lose": 75,
        "lose_much": 124
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.2648305084745763,
      "Information/Advice seeking": -0.15655339805825244,
      "Coding & Debugging": -0.3439153439153439,
      "Creative Tasks": -0.031512605042016806,
      "Math & Data Analysis": -0.42907801418439717
    },
    "reward": -0.18310546875,
    "task_macro_reward": -0.27493396550170207,
    "K": 1000
  },
  "Yi-1.5-6B-Chat": {
    "model": "Yi-1.5-6B-Chat",
    "win_much": 111,
    "win": 128,
    "tie": 162,
    "lose": 243,
    "lose_much": 248,
    "total": 1023,
    "avg_len": 2959.330717488789,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 75,
        "win": 93,
        "tie": 137,
        "lose": 192,
        "lose_much": 181
      },
      "Information/Advice seeking": {
        "win_much": 43,
        "win": 61,
        "tie": 89,
        "lose": 114,
        "lose_much": 87
      },
      "Coding & Debugging": {
        "win_much": 14,
        "win": 26,
        "tie": 17,
        "lose": 45,
        "lose_much": 77
      },
      "Creative Tasks": {
        "win_much": 56,
        "win": 56,
        "tie": 98,
        "lose": 141,
        "lose_much": 90
      },
      "Math & Data Analysis": {
        "win_much": 45,
        "win": 38,
        "tie": 34,
        "lose": 65,
        "lose_much": 94
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.22935103244837757,
      "Information/Advice seeking": -0.17893401015228427,
      "Coding & Debugging": -0.40502793296089384,
      "Creative Tasks": -0.17346938775510204,
      "Math & Data Analysis": -0.22644927536231885
    },
    "reward": -0.19012707722385142,
    "task_macro_reward": -0.25313360008343305,
    "K": 1000
  },
  "Mistral-7B-Instruct-v0.2": {
    "model": "Mistral-7B-Instruct-v0.2",
    "win_much": 99,
    "win": 165,
    "tie": 125,
    "lose": 331,
    "lose_much": 219,
    "total": 1024,
    "avg_len": 2478.094781682641,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 66,
        "win": 122,
        "tie": 70,
        "lose": 158,
        "lose_much": 57
      },
      "Coding & Debugging": {
        "win_much": 18,
        "win": 24,
        "tie": 15,
        "lose": 53,
        "lose_much": 72
      },
      "Planning & Reasoning": {
        "win_much": 58,
        "win": 104,
        "tie": 110,
        "lose": 282,
        "lose_much": 166
      },
      "Information/Advice seeking": {
        "win_much": 47,
        "win": 71,
        "tie": 67,
        "lose": 167,
        "lose_much": 69
      },
      "Math & Data Analysis": {
        "win_much": 26,
        "win": 24,
        "tie": 21,
        "lose": 105,
        "lose_much": 99
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": -0.019027484143763214,
      "Coding & Debugging": -0.37637362637362637,
      "Planning & Reasoning": -0.27361111111111114,
      "Information/Advice seeking": -0.166270783847981,
      "Math & Data Analysis": -0.4127272727272727
    },
    "reward": -0.1982421875,
    "task_macro_reward": -0.28118480398836787,
    "K": 1000
  },
  "Qwen1.5-7B-Chat@together": {
    "model": "Qwen1.5-7B-Chat@together",
    "win_much": 92,
    "win": 205,
    "tie": 103,
    "lose": 308,
    "lose_much": 249,
    "total": 1022,
    "avg_len": 2364.1264367816093,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 69,
        "win": 131,
        "tie": 63,
        "lose": 156,
        "lose_much": 67
      },
      "Coding & Debugging": {
        "win_much": 20,
        "win": 30,
        "tie": 9,
        "lose": 54,
        "lose_much": 71
      },
      "Planning & Reasoning": {
        "win_much": 55,
        "win": 155,
        "tie": 97,
        "lose": 242,
        "lose_much": 176
      },
      "Information/Advice seeking": {
        "win_much": 36,
        "win": 93,
        "tie": 54,
        "lose": 162,
        "lose_much": 85
      },
      "Math & Data Analysis": {
        "win_much": 24,
        "win": 39,
        "tie": 28,
        "lose": 85,
        "lose_much": 108
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": -0.021604938271604937,
      "Coding & Debugging": -0.3423913043478261,
      "Planning & Reasoning": -0.22689655172413792,
      "Information/Advice seeking": -0.1941860465116279,
      "Math & Data Analysis": -0.3767605633802817
    },
    "reward": -0.20401174168297456,
    "task_macro_reward": -0.25760323586728967,
    "K": 1000
  },
  "Llama-2-70b-chat-hf": {
    "model": "Llama-2-70b-chat-hf",
    "win_much": 96,
    "win": 140,
    "tie": 142,
    "lose": 301,
    "lose_much": 254,
    "total": 1023,
    "avg_len": 2761.801714898178,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 60,
        "win": 112,
        "tie": 109,
        "lose": 222,
        "lose_much": 202
      },
      "Information/Advice seeking": {
        "win_much": 46,
        "win": 79,
        "tie": 80,
        "lose": 155,
        "lose_much": 64
      },
      "Coding & Debugging": {
        "win_much": 9,
        "win": 17,
        "tie": 19,
        "lose": 43,
        "lose_much": 92
      },
      "Creative Tasks": {
        "win_much": 70,
        "win": 74,
        "tie": 84,
        "lose": 169,
        "lose_much": 63
      },
      "Math & Data Analysis": {
        "win_much": 25,
        "win": 18,
        "tie": 29,
        "lose": 79,
        "lose_much": 133
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.2794326241134752,
      "Information/Advice seeking": -0.1320754716981132,
      "Coding & Debugging": -0.5333333333333333,
      "Creative Tasks": -0.08804347826086957,
      "Math & Data Analysis": -0.4876760563380282
    },
    "reward": -0.23313782991202345,
    "task_macro_reward": -0.3363063361593834,
    "K": 1000
  },
  "gpt-3.5-turbo-0125": {
    "model": "gpt-3.5-turbo-0125",
    "win_much": 80,
    "win": 194,
    "tie": 112,
    "lose": 297,
    "lose_much": 286,
    "total": 1024,
    "avg_len": 1747.4912280701753,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 48,
        "win": 124,
        "tie": 72,
        "lose": 150,
        "lose_much": 93
      },
      "Coding & Debugging": {
        "win_much": 21,
        "win": 35,
        "tie": 20,
        "lose": 36,
        "lose_much": 79
      },
      "Planning & Reasoning": {
        "win_much": 49,
        "win": 136,
        "tie": 98,
        "lose": 245,
        "lose_much": 211
      },
      "Information/Advice seeking": {
        "win_much": 27,
        "win": 76,
        "tie": 47,
        "lose": 164,
        "lose_much": 118
      },
      "Math & Data Analysis": {
        "win_much": 28,
        "win": 50,
        "tie": 23,
        "lose": 74,
        "lose_much": 116
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": -0.11909650924024641,
      "Coding & Debugging": -0.306282722513089,
      "Planning & Reasoning": -0.2929634641407307,
      "Information/Advice seeking": -0.3125,
      "Math & Data Analysis": -0.3436426116838488
    },
    "reward": -0.25146484375,
    "task_macro_reward": -0.29112287088732763,
    "K": 1000
  },
  "Phi-3-medium-128k-instruct": {
    "model": "Phi-3-medium-128k-instruct",
    "win_much": 87,
    "win": 152,
    "tie": 111,
    "lose": 330,
    "lose_much": 256,
    "total": 1024,
    "avg_len": 2262.357905982906,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 53,
        "win": 88,
        "tie": 64,
        "lose": 156,
        "lose_much": 99
      },
      "Coding & Debugging": {
        "win_much": 19,
        "win": 21,
        "tie": 22,
        "lose": 58,
        "lose_much": 66
      },
      "Planning & Reasoning": {
        "win_much": 51,
        "win": 106,
        "tie": 95,
        "lose": 268,
        "lose_much": 202
      },
      "Information/Advice seeking": {
        "win_much": 26,
        "win": 68,
        "tie": 49,
        "lose": 165,
        "lose_much": 108
      },
      "Math & Data Analysis": {
        "win_much": 39,
        "win": 54,
        "tie": 18,
        "lose": 92,
        "lose_much": 78
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": -0.17391304347826086,
      "Coding & Debugging": -0.3521505376344086,
      "Planning & Reasoning": -0.32132963988919666,
      "Information/Advice seeking": -0.3137019230769231,
      "Math & Data Analysis": -0.20640569395017794
    },
    "reward": -0.251953125,
    "task_macro_reward": -0.28498916566509,
    "K": 1000
  },
  "Magpie-Pro-SFT-v0.1": {
    "model": "Magpie-Pro-SFT-v0.1",
    "win_much": 78,
    "win": 127,
    "tie": 147,
    "lose": 281,
    "lose_much": 267,
    "total": 1023,
    "avg_len": 2699.12,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 48,
        "win": 72,
        "tie": 85,
        "lose": 147,
        "lose_much": 112
      },
      "Coding & Debugging": {
        "win_much": 14,
        "win": 17,
        "tie": 12,
        "lose": 48,
        "lose_much": 74
      },
      "Planning & Reasoning": {
        "win_much": 53,
        "win": 94,
        "tie": 118,
        "lose": 214,
        "lose_much": 202
      },
      "Information/Advice seeking": {
        "win_much": 32,
        "win": 54,
        "tie": 97,
        "lose": 143,
        "lose_much": 85
      },
      "Math & Data Analysis": {
        "win_much": 25,
        "win": 46,
        "tie": 22,
        "lose": 72,
        "lose_much": 98
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": -0.21875,
      "Coding & Debugging": -0.4575757575757576,
      "Planning & Reasoning": -0.3069016152716593,
      "Information/Advice seeking": -0.23722627737226276,
      "Math & Data Analysis": -0.3269961977186312
    },
    "reward": -0.260019550342131,
    "task_macro_reward": -0.3234430405362578,
    "K": 1000
  },
  "Phi-3-mini-128k-instruct": {
    "model": "Phi-3-mini-128k-instruct",
    "win_much": 79,
    "win": 156,
    "tie": 110,
    "lose": 277,
    "lose_much": 326,
    "total": 1023,
    "avg_len": 2140.9535864978902,
    "task_categorized_results": {
      "Creative Tasks": {
        "win_much": 47,
        "win": 78,
        "tie": 80,
        "lose": 142,
        "lose_much": 129
      },
      "Coding & Debugging": {
        "win_much": 20,
        "win": 36,
        "tie": 14,
        "lose": 46,
        "lose_much": 71
      },
      "Planning & Reasoning": {
        "win_much": 52,
        "win": 108,
        "tie": 91,
        "lose": 232,
        "lose_much": 242
      },
      "Information/Advice seeking": {
        "win_much": 16,
        "win": 64,
        "tie": 55,
        "lose": 129,
        "lose_much": 154
      },
      "Math & Data Analysis": {
        "win_much": 33,
        "win": 41,
        "tie": 28,
        "lose": 78,
        "lose_much": 107
      }
    },
    "task_categorized_rewards": {
      "Creative Tasks": -0.23949579831932774,
      "Coding & Debugging": -0.2994652406417112,
      "Planning & Reasoning": -0.34758620689655173,
      "Information/Advice seeking": -0.40789473684210525,
      "Math & Data Analysis": -0.32229965156794427
    },
    "reward": -0.30058651026392963,
    "task_macro_reward": -0.32931480058257506,
    "K": 1000
  },
  "Llama-2-7b-chat-hf": {
    "model": "Llama-2-7b-chat-hf",
    "win_much": 41,
    "win": 99,
    "tie": 119,
    "lose": 312,
    "lose_much": 357,
    "total": 1023,
    "avg_len": 2628.8588362068967,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 22,
        "win": 67,
        "tie": 77,
        "lose": 256,
        "lose_much": 273
      },
      "Information/Advice seeking": {
        "win_much": 21,
        "win": 47,
        "tie": 72,
        "lose": 164,
        "lose_much": 116
      },
      "Coding & Debugging": {
        "win_much": 4,
        "win": 10,
        "tie": 7,
        "lose": 40,
        "lose_much": 115
      },
      "Creative Tasks": {
        "win_much": 32,
        "win": 73,
        "tie": 76,
        "lose": 177,
        "lose_much": 113
      },
      "Math & Data Analysis": {
        "win_much": 7,
        "win": 20,
        "tie": 18,
        "lose": 71,
        "lose_much": 162
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.49712230215827335,
      "Information/Advice seeking": -0.36547619047619045,
      "Coding & Debugging": -0.7159090909090909,
      "Creative Tasks": -0.2823779193205945,
      "Math & Data Analysis": -0.6492805755395683
    },
    "reward": -0.41300097751710657,
    "task_macro_reward": -0.5337530325919869,
    "K": 1000
  },
  "gemma-7b-it": {
    "model": "gemma-7b-it",
    "win_much": 30,
    "win": 122,
    "tie": 92,
    "lose": 245,
    "lose_much": 482,
    "total": 1024,
    "avg_len": 1670.7322348094747,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 18,
        "win": 83,
        "tie": 85,
        "lose": 186,
        "lose_much": 368
      },
      "Information/Advice seeking": {
        "win_much": 6,
        "win": 66,
        "tie": 36,
        "lose": 120,
        "lose_much": 204
      },
      "Coding & Debugging": {
        "win_much": 6,
        "win": 16,
        "tie": 24,
        "lose": 29,
        "lose_much": 115
      },
      "Creative Tasks": {
        "win_much": 26,
        "win": 81,
        "tie": 54,
        "lose": 145,
        "lose_much": 183
      },
      "Math & Data Analysis": {
        "win_much": 7,
        "win": 20,
        "tie": 27,
        "lose": 61,
        "lose_much": 176
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.5425675675675675,
      "Information/Advice seeking": -0.5208333333333334,
      "Coding & Debugging": -0.6078947368421053,
      "Creative Tasks": -0.38650306748466257,
      "Math & Data Analysis": -0.6512027491408935
    },
    "reward": -0.50146484375,
    "task_macro_reward": -0.5602629953743976,
    "K": 1000
  },
  "gemma-2b-it": {
    "model": "gemma-2b-it",
    "win_much": 14,
    "win": 53,
    "tie": 75,
    "lose": 196,
    "lose_much": 633,
    "total": 1024,
    "avg_len": 1520.9011328527292,
    "task_categorized_results": {
      "Planning & Reasoning": {
        "win_much": 10,
        "win": 29,
        "tie": 66,
        "lose": 152,
        "lose_much": 483
      },
      "Information/Advice seeking": {
        "win_much": 2,
        "win": 25,
        "tie": 31,
        "lose": 81,
        "lose_much": 293
      },
      "Coding & Debugging": {
        "win_much": 0,
        "win": 9,
        "tie": 16,
        "lose": 26,
        "lose_much": 139
      },
      "Creative Tasks": {
        "win_much": 17,
        "win": 36,
        "tie": 45,
        "lose": 137,
        "lose_much": 254
      },
      "Math & Data Analysis": {
        "win_much": 3,
        "win": 14,
        "tie": 18,
        "lose": 45,
        "lose_much": 211
      }
    },
    "task_categorized_rewards": {
      "Planning & Reasoning": -0.7222972972972973,
      "Information/Advice seeking": -0.7384259259259259,
      "Coding & Debugging": -0.7763157894736842,
      "Creative Tasks": -0.5879345603271984,
      "Math & Data Analysis": -0.7680412371134021
    },
    "reward": -0.67431640625,
    "task_macro_reward": -0.7322256384037226,
    "K": 1000
  }
}